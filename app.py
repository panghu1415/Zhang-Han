import streamlit as st
import streamlit.components.v1 as components
import pandas as pd
import json
import requests
import re
import base64
import plotly.graph_objects as go
import plotly.express as px
from typing import List, Dict, Any, Optional, Callable
import concurrent.futures
from io import BytesIO
import numpy as np  # âœ… å•ç‹¬å¼•å…¥ numpy

# ================== å…è®¸çš„æµ‹è¯•ç”¨ä¾‹ç±»å‹ ==================
ALLOWED_TYPES = ["æ­£å‘", "å¼‚å¸¸", "è¾¹ç•Œ", "å®‰å…¨", "æ€§èƒ½", "ç•Œé¢", "å…¶ä»–"]

# æ¯ä¸ªåŠŸèƒ½ç‚¹æœ€å¤šå¸¦å…¥å¤šå°‘å­—ç¬¦çš„ä¸Šä¸‹æ–‡ï¼Œé˜²æ­¢ PRD å¾ˆé•¿æ—¶æ¯æ¬¡éƒ½å¡æ•´ç¯‡
MAX_CONTEXT_CHARS = 2000

# ================== MarkMap æ€ç»´å¯¼å›¾ï¼ˆå¯é€‰ï¼‰ ==================
try:
    from streamlit_markmap import markmap
    HAS_MARKMAP = True
except ImportError:
    HAS_MARKMAP = False

# ================== é¡µé¢åŸºç¡€é…ç½®ï¼ˆâš  å¿…é¡»æ˜¯ç¬¬ä¸€ä¸ª st.* è°ƒç”¨ï¼‰ ==================
st.set_page_config(
    page_title="æ™ºæµ‹ AI Pro - éœ€æ±‚è½¬ç”¨ä¾‹å·¥ä½œå°ï¼ˆå¼ºåŒ–ç‰ˆï¼‰",
    page_icon="ğŸ§¬",
    layout="wide",
    initial_sidebar_state="expanded",
)

# ================== è¯­ä¹‰ç›¸ä¼¼åº¦ Embeddingï¼ˆå¯é€‰ï¼Œä½¿ç”¨ç¼“å­˜ï¼‰ ==================
@st.cache_resource
def load_embedding_model():
    """
    åªåœ¨ç¬¬ä¸€æ¬¡è°ƒç”¨æ—¶åŠ è½½ SentenceTransformerï¼Œåé¢éƒ½èµ°ç¼“å­˜ã€‚
    """
    try:
        from sentence_transformers import SentenceTransformer
        return SentenceTransformer("sentence-transformers/all-MiniLM-L6-v2")
    except Exception:
        return None

EMBED_MODEL = load_embedding_model()
HAS_EMBED = EMBED_MODEL is not None

# ================== JSON Repairï¼ˆå¯é€‰ï¼‰ ==================
try:
    from json_repair import repair_json
    HAS_JSON_REPAIR = True
except Exception:
    HAS_JSON_REPAIR = False

# ================== é€šç”¨å·¥å…·å‡½æ•° ==================


def clean_and_parse_json(text: str) -> Any:
    """
    å¼ºåŠ› JSON æ¸…æ´—ï¼š
    1. ç›´æ¥ json.loads
    2. ä½¿ç”¨ json_repair ä¿®å¤
    3. æå– ```json ... ``` ä¸­é—´çš„å†…å®¹
    4. æˆªå–ç¬¬ä¸€ä¸ª { åˆ°æœ€åä¸€ä¸ª } ä¹‹é—´çš„å†…å®¹
    """
    if not isinstance(text, str):
        raise ValueError("æ¨¡å‹è¿”å›å†…å®¹ä¸æ˜¯å­—ç¬¦ä¸²")

    # 1. ç›´æ¥è§£æ
    try:
        return json.loads(text)
    except Exception:
        pass

    # 2. ä½¿ç”¨ json_repair ä¿®å¤
    if HAS_JSON_REPAIR:
        try:
            repaired = repair_json(text)
            return json.loads(repaired)
        except Exception:
            pass

    # 3. ```json ... ```
    match = re.search(r"```json\s*(.*?)\s*```", text, re.DOTALL)
    if match:
        snippet = match.group(1)
        # å…ˆè¯•ç›´æ¥ load
        try:
            return json.loads(snippet)
        except Exception:
            # å†å°è¯• repair
            if HAS_JSON_REPAIR:
                try:
                    repaired = repair_json(snippet)
                    return json.loads(repaired)
                except Exception:
                    pass

    # 4. ä»ç¬¬ä¸€ä¸ª { åˆ°æœ€åä¸€ä¸ª }
    start = text.find("{")
    end = text.rfind("}") + 1
    if start != -1 and end != -1 and end > start:
        snippet = text[start:end]
        try:
            return json.loads(snippet)
        except Exception:
            if HAS_JSON_REPAIR:
                try:
                    repaired = repair_json(snippet)
                    return json.loads(repaired)
                except Exception:
                    pass

    raise ValueError(f"æ— æ³•æå–æœ‰æ•ˆ JSONï¼ŒåŸå§‹è¿”å›å¼€å¤´ä¸º: {text[:200]}...")


def get_feishu_content(url: str, app_id: str, app_secret: str) -> str:
    """
    é£ä¹¦æ–‡æ¡£è§£æï¼š
    - æœªé…ç½® app_id / secret æ—¶ï¼šè¿”å› Mock PRD å†…å®¹ï¼Œä¿è¯ Demo ä¸ç¿»è½¦
    - é…ç½®åï¼šå°è¯•è°ƒç”¨é£ä¹¦ APIï¼ˆç®€åŒ–ç‰ˆï¼‰
    - å¯¹ Table Block å°è¯•è½¬æ¢ä¸º Markdown è¡¨æ ¼
    """
    if not url:
        return ""

    mock_content = """
# [æ¨¡æ‹Ÿ] Bç«¯ç®¡ç†åå°ç™»å½•åŠŸèƒ½

## 1. è´¦å·ç™»å½•
ç”¨æˆ·éœ€è¾“å…¥æ‰‹æœºå·å’Œå¯†ç ã€‚æ‰‹æœºå·éœ€éªŒè¯ 11 ä½æ ¼å¼ã€‚

## 2. å¼‚å¸¸å¤„ç†
- å¯†ç é”™è¯¯è¶…è¿‡ 5 æ¬¡é”å®šè´¦å· 30 åˆ†é’Ÿã€‚
- ç½‘ç»œæ–­å¼€æ—¶åº”æç¤ºâ€œç½‘ç»œè¿æ¥å¼‚å¸¸â€ã€‚
""".strip()

    if not app_id or not app_secret:
        return f"ã€æ¼”ç¤ºæ¨¡å¼ - æœªé…ç½®é£ä¹¦ Keyã€‘\nå·²æ¨¡æ‹Ÿè¯»å–æ–‡æ¡£ï¼š{url}\n\n{mock_content}"

    try:
        # 1. è·å– tenant_access_token
        token_url = "https://open.feishu.cn/open-apis/auth/v3/tenant_access_token/internal"
        token_resp = requests.post(
            token_url,
            json={"app_id": app_id, "app_secret": app_secret},
            timeout=15,
        ).json()

        if token_resp.get("code") != 0:
            return f"âŒ é£ä¹¦é‰´æƒå¤±è´¥: {token_resp.get('msg')}"

        access_token = token_resp["tenant_access_token"]

        # 2. è§£æ doc_token
        doc_token = url.rstrip("/").split("/")[-1].split("?")[0]

        # 3. è·å– blocksï¼ˆä»…æ¼”ç¤ºï¼‰
        content_url = f"https://open.feishu.cn/open-apis/docx/v1/documents/{doc_token}/blocks"
        headers = {"Authorization": f"Bearer {access_token}"}
        resp = requests.get(content_url, headers=headers, timeout=15).json()

        if resp.get("code") != 0:
            return f"âŒ æ–‡æ¡£è¯»å–å¤±è´¥: {resp.get('msg')}"

        full_text_lines = []

        for item in resp.get("data", {}).get("items", []):
            block_type = item.get("block_type")

            # æ–‡æœ¬å—
            if block_type == 2:
                for elem in item.get("body", {}).get("elements", []):
                    content = elem.get("text_run", {}).get("content", "")
                    if content:
                        full_text_lines.append(content)

            # ç®€å•å¤„ç†è¡¨æ ¼å—ï¼šå°è¯•è½¬æˆ Markdown è¡¨æ ¼
            elif block_type == 3:
                table = item.get("table", {})
                rows = table.get("cells") or table.get("rows") or []
                md_rows = []
                try:
                    for r in rows:
                        # ä¸åŒç‰ˆæœ¬ç»“æ„å¯èƒ½ä¸ä¸€æ ·ï¼Œè¿™é‡Œåšå°½é‡â€œé˜²å¾¡å¼â€çš„è§£æ
                        cells = r.get("cells") if isinstance(r, dict) else r
                        row_texts = []
                        for cell in cells:
                            cell_text = ""
                            for elem in cell.get("body", {}).get("elements", []):
                                cell_text += elem.get("text_run", {}).get("content", "")
                            row_texts.append(cell_text.strip() or " ")
                        md_rows.append("| " + " | ".join(row_texts) + " |")
                    if md_rows:
                        # ç®€å•åŠ  header åˆ†å‰²çº¿
                        if len(md_rows) >= 2:
                            col_num = md_rows[0].count("|") - 1
                            sep = "| " + " | ".join(["---"] * col_num) + " |"
                            full_text_lines.append(md_rows[0])
                            full_text_lines.append(sep)
                            full_text_lines.extend(md_rows[1:])
                        else:
                            full_text_lines.extend(md_rows)
                except Exception:
                    # å¦‚æœè¡¨æ ¼è§£æå¤±è´¥ï¼Œä¸ä¸­æ–­æ•´ä½“é€»è¾‘
                    pass

        full_text = "\n".join(full_text_lines)
        return full_text or "æ–‡æ¡£å†…å®¹ä¸ºç©ºæˆ–è§£æå¤±è´¥"

    except Exception as e:
        return f"âš ï¸ æ¥å£è°ƒç”¨å¼‚å¸¸ (å·²åˆ‡æ¢è‡³æ¨¡æ‹Ÿæ•°æ®): {str(e)}\n\n{mock_content}"


def call_llm(
    api_key: str,
    model_id: str,
    messages: List[Dict[str, Any]],
    response_format: Optional[Dict[str, Any]] = None,
    timeout: int = 300,
) -> str:
    """
    é€šç”¨ LLM è°ƒç”¨å°è£…ï¼š
    - ä½¿ç”¨ Ark ChatCompletions
    - é»˜è®¤è¿”å› message.content å­—ç¬¦ä¸²
    """
    if not api_key:
        raise RuntimeError("æœªé…ç½® API Key")

    url = "https://ark.cn-beijing.volces.com/api/v3/chat/completions"

    payload: Dict[str, Any] = {
        "model": model_id,
        "messages": messages,
        "temperature": 0.2,
        "stream": False,
    }
    if response_format is not None:
        payload["response_format"] = response_format

    headers = {
        "Authorization": f"Bearer {api_key}",
        "Content-Type": "application/json",
    }

    try:
        session = requests.Session()
        session.trust_env = False
        resp = session.post(url, headers=headers, json=payload, timeout=timeout)
    except Exception as e:
        raise RuntimeError(f"è°ƒç”¨ LLM ç½‘ç»œå¼‚å¸¸ï¼š{e}")

    if resp.status_code != 200:
        raise RuntimeError(f"API Error {resp.status_code}: {resp.text}")

    data = resp.json()
    return data["choices"][0]["message"]["content"]


# ================== priority åå¤„ç† ==================


def post_process_priority(features: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    """
    ç®€å•æŠŠ priority åˆ†å±‚ï¼š
    - å¦‚æœæ¨¡å‹å·²ç»ç»™äº† P0/P1/P2ï¼Œå°±å…ˆä¿ç•™ï¼›
    - å¦‚æœå…¨éƒ¨éƒ½æ˜¯ P0 æˆ–éå¸¸é›†ä¸­ï¼Œå°±æŒ‰é¡ºåºé‡æ–°åˆ’åˆ†æ¯”ä¾‹ã€‚
    """
    n = len(features)
    if n == 0:
        return features

    pri_list = [f.get("priority", "P1") for f in features]
    unique_pri = set(pri_list)

    if len(unique_pri) <= 1:
        # æŒ‰åºç®€å•åˆ†æ¡£
        for idx, f in enumerate(features):
            ratio = (idx + 1) / n
            if ratio <= 0.3:
                f["priority"] = "P0"
            elif ratio <= 0.7:
                f["priority"] = "P1"
            else:
                f["priority"] = "P2"
    else:
        for f in features:
            p = f.get("priority", "P1")
            if p not in ["P0", "P1", "P2"]:
                f["priority"] = "P1"

    return features


# ================== CoT / åˆ†æ²»ï¼šåŠŸèƒ½ç‚¹ + åˆ†æ²»ç”Ÿæˆ ==================


def extract_features(prd_text: str, guidelines: str, api_key: str, model_id: str) -> List[Dict[str, Any]]:
    """
    é˜¶æ®µä¸€ï¼šä» PRD ä¸­æŠ½å–åŠŸèƒ½ç‚¹ï¼ˆfeaturesï¼‰
    è¿”å›ç±»ä¼¼ï¼š
    [{
      "id":"F1",
      "name":"ç™»å½•æˆåŠŸ",
      "desc":"...",
      "priority":"P0",
      "module":"ç™»å½•æ¨¡å—",
      "scene_type":"æ­£å‘" / "å¼‚å¸¸" / "çº¦æŸ" / "è¾¹ç•Œ" / "å®‰å…¨" / "å…¶ä»–",
      "source_text":"æ¥è‡ª PRD çš„å…³é”®åŸæ–‡ç‰‡æ®µï¼Œç”¨äºç¼©çŸ­åç»­ä¸Šä¸‹æ–‡"
    }, ...]
    """
    guideline_text = guidelines.strip() or "æ— "

    prompt = f"""
ä½ æ˜¯ä¸€åèµ„æ·±æµ‹è¯•åˆ†æå¸ˆï¼Œè¯·ä»ä»¥ä¸‹ PRD ä¸­æŠ½å–åŠŸèƒ½ç‚¹åˆ—è¡¨ï¼Œä»¥ä¾¿åç»­ä¸ºæ¯ä¸ªåŠŸèƒ½ç‚¹è®¾è®¡æµ‹è¯•ç”¨ä¾‹ã€‚

ã€é‡è¦è¦æ±‚ã€‘
- ä½ çš„åˆ†æå’Œè¾“å‡ºå¯ä»¥ä½¿ç”¨ä¸­æ–‡æˆ–è‹±æ–‡ï¼Œä½†æœ€ç»ˆ JSON ä¸­çš„å­—æ®µå€¼ï¼ˆåŠŸèƒ½ç‚¹åç§°ã€æè¿°ã€æ¨¡å—åã€scene_typeã€source_textï¼‰ä¸€å¾‹ä½¿ç”¨ç®€ä½“ä¸­æ–‡ã€‚
- JSON çš„ key ä½¿ç”¨è‹±æ–‡ï¼ˆå¦‚ "id"ã€"name"ã€"scene_type"ã€"source_text"ï¼‰ï¼Œvalue ä½¿ç”¨ä¸­æ–‡ã€‚
- æ¯ä¸ªåŠŸèƒ½ç‚¹è¯·å¢åŠ  scene_type å­—æ®µï¼Œå–å€¼ä¹‹ä¸€ï¼š
  - "æ­£å‘"ï¼šæ­£å¸¸ä¸šåŠ¡ä¸»æµç¨‹ï¼Œå¦‚ç™»å½•æˆåŠŸã€ä¸‹å•æˆåŠŸç­‰
  - "å¼‚å¸¸"ï¼šé”™è¯¯åœºæ™¯æˆ–å¼‚å¸¸åˆ†æ”¯ï¼Œå¦‚è´¦å·ä¸å­˜åœ¨ã€å¯†ç é”™è¯¯ã€æƒé™ä¸è¶³ç­‰
  - "çº¦æŸ"ï¼šä¸šåŠ¡çº¦æŸ/è§„åˆ™ï¼Œå¦‚â€œç”¨æˆ·åé•¿åº¦å¿…é¡» 1~20 ä½â€â€œé‡‘é¢ä¸å¾—ä¸ºè´Ÿæ•°â€ç­‰
  - "è¾¹ç•Œ"ï¼šä¸“é—¨æè¿°è¾¹ç•Œå€¼/ä¸´ç•Œå€¼è§„åˆ™çš„åŠŸèƒ½ç‚¹
  - "å®‰å…¨"ï¼šä¸å®‰å…¨ã€æƒé™ã€é£æ§ç›´æ¥ç›¸å…³çš„åŠŸèƒ½ç‚¹
  - å…¶ä»–æƒ…å†µå¯ä»¥ç”¨ "å…¶ä»–"
- æ¯ä¸ªåŠŸèƒ½ç‚¹å°½é‡è¡¥å……ä¸€ä¸ª source_text å­—æ®µï¼šç›´æ¥ä» PRD ä¸­å¤åˆ¶ä¸è¯¥åŠŸèƒ½ç‚¹æœ€ç›¸å…³çš„åŸæ–‡æ®µè½æˆ–å°èŠ‚ï¼Œç”¨äºåç»­ç¼©çŸ­ä¸Šä¸‹æ–‡ã€‚

ã€è¾“å…¥ã€‘
1. PRD æ–‡æœ¬ï¼š
{prd_text}

2. ä¼ä¸šæµ‹è¯•è§„èŒƒï¼ˆå¯é€‰ï¼‰ï¼š
{guideline_text}

ã€è¾“å‡ºè¦æ±‚ã€‘
- åªè¾“å‡º JSONï¼Œä¸€å®šè¦æ˜¯åˆæ³•çš„ JSON å¯¹è±¡ã€‚
- JSON æ ¼å¼ç¤ºä¾‹ï¼š
{{
  "features": [
    {{
      "id": "F1",
      "name": "ç™»å½•æˆåŠŸ",
      "desc": "å·²æ³¨å†Œç”¨æˆ·è¾“å…¥æ­£ç¡®çš„è´¦å·å’Œå¯†ç ç™»å½•ç³»ç»Ÿå¹¶è¿›å…¥é¦–é¡µã€‚",
      "priority": "P0",         // P0/P1/P2
      "module": "ç”¨æˆ·ç™»å½•",      // å¯å¤ç”¨ PRD ä¸­çš„æ¨¡å—/é¡µé¢å
      "scene_type": "æ­£å‘",
      "source_text": "ä» PRD ä¸­å¤åˆ¶æ¥çš„ç›¸å…³åŸæ–‡"
    }}
  ]
}}
- è¯·å°½é‡è¦†ç›– PRD ä¸­æ‰€æœ‰ä¸»è¦åŠŸèƒ½ç‚¹ï¼ˆåŒ…æ‹¬æ˜æ˜¾çš„å¼‚å¸¸/çº¦æŸ/è¾¹ç•Œè§„åˆ™ï¼‰ï¼Œä¸€èˆ¬ä¸è¶…è¿‡ 20 ä¸ªåŠŸèƒ½ç‚¹ã€‚
""".strip()

    messages = [
        {"role": "system", "content": "ä½ æ˜¯ä¸€åä¸¥è°¨çš„æµ‹è¯•åˆ†æå¸ˆï¼Œè´Ÿè´£æ‹†è§£ PRD åŠŸèƒ½ç‚¹ï¼Œè¯·ç”¨ç®€ä½“ä¸­æ–‡è¾“å‡ºå­—æ®µå€¼ã€‚"},
        {"role": "user", "content": prompt},
    ]

    raw = call_llm(
        api_key=api_key,
        model_id=model_id,
        messages=messages,
        response_format={"type": "json_object"},
        timeout=180,
    )
    obj = clean_and_parse_json(raw)
    features = obj.get("features", [])
    norm_features: List[Dict[str, Any]] = []

    for idx, f in enumerate(features, start=1):
        norm_features.append(
            {
                "id": f.get("id", f"F{idx}"),
                "name": f.get("name", f"åŠŸèƒ½ç‚¹ {idx}"),
                "desc": f.get("desc", ""),
                "priority": f.get("priority", "P1"),
                "module": f.get("module", f.get("name", "æœªåˆ†æ¨¡å—")),
                "scene_type": f.get("scene_type", "æ­£å‘"),
                "source_text": f.get("source_text", ""),
            }
        )

    norm_features = post_process_priority(norm_features)
    return norm_features


def normalize_case_type(raw_type: str) -> str:
    """
    å°†æ¨¡å‹è¿”å›çš„ type å½’ä¸€åŒ–åˆ° ALLOWED_TYPES ä¸­ï¼š
    - ä¼˜å…ˆåŒ¹é…ä¸­æ–‡
    - å¸¸è§è‹±æ–‡æ˜ å°„
    - å…¶ä»–å½’ä¸º "å…¶ä»–"
    """
    if not raw_type:
        return "å…¶ä»–"
    t = str(raw_type).strip()

    if t in ALLOWED_TYPES:
        return t

    # ä¸­æ–‡åˆ«å
    if t in ["æ­£å¸¸", "ä¸»æµç¨‹", "æ­£å‘ç”¨ä¾‹"]:
        return "æ­£å‘"
    if t in ["å¼‚å¸¸ç”¨ä¾‹", "é”™è¯¯", "å¤±è´¥åœºæ™¯"]:
        return "å¼‚å¸¸"
    if t in ["è¾¹ç•Œå€¼", "è¾¹ç•Œæµ‹è¯•"]:
        return "è¾¹ç•Œ"
    if t in ["UI", "ç•Œé¢æµ‹è¯•"]:
        return "ç•Œé¢"

    # è‹±æ–‡å¸¸è§å€¼
    lower = t.lower()
    if lower in ["positive", "happy path", "success"]:
        return "æ­£å‘"
    if lower in ["negative", "error", "exception"]:
        return "å¼‚å¸¸"
    if "boundary" in lower:
        return "è¾¹ç•Œ"
    if "security" in lower:
        return "å®‰å…¨"
    if "performance" in lower:
        return "æ€§èƒ½"
    if lower in ["ui", "ux"]:
        return "ç•Œé¢"

    return "å…¶ä»–"


def normalize_cases(json_obj: Any) -> List[Dict[str, Any]]:
    """
    å°†æ¨¡å‹è¿”å›çš„ JSONï¼ˆ{"cases": [...]} æˆ–ç›´æ¥ [... ]ï¼‰ç»Ÿä¸€ä¸ºï¼š
    {
      "id": "TC-001",
      "module": "...",
      "title": "...",
      "precondition": "...",
      "steps": "...(å¤šè¡Œ)",
      "expected": "...(å¤šè¡Œ)",
      "type": one of ALLOWED_TYPES,
      "test_data": "...(JSON æˆ–æè¿°)",
      "post_actions": "...(æ¸…ç†/å›æ»šæ­¥éª¤)"
    }
    """
    if isinstance(json_obj, dict) and "cases" in json_obj:
        raw_cases = json_obj["cases"]
    elif isinstance(json_obj, list):
        raw_cases = json_obj
    else:
        raise ValueError("è¿”å› JSON ä¸­æœªæ‰¾åˆ° 'cases' åˆ—è¡¨")

    norm: List[Dict[str, Any]] = []
    for idx, c in enumerate(raw_cases, start=1):
        module = c.get("module", "æœªåˆ†æ¨¡å—")
        title = c.get("title", f"æœªå‘½åç”¨ä¾‹ {idx}")
        precondition = c.get("precondition", "")
        steps = c.get("steps", "")
        expected = c.get("expected", "")
        raw_type = c.get("type", "æ­£å‘")
        test_data = c.get("test_data", "")
        post_actions = c.get("post_actions", "") or c.get("teardown", "")

        if isinstance(steps, list):
            steps = "\n".join(str(s) for s in steps)
        if isinstance(expected, list):
            expected = "\n".join(str(s) for s in expected)

        if isinstance(test_data, (dict, list)):
            try:
                test_data = json.dumps(test_data, ensure_ascii=False, indent=2)
            except Exception:
                test_data = str(test_data)
        else:
            test_data = str(test_data)

        if isinstance(post_actions, list):
            post_actions = "\n".join(str(s) for s in post_actions)
        else:
            post_actions = str(post_actions)

        norm.append(
            {
                "id": c.get("id", f"TC-{idx:03d}"),
                "module": module,
                "title": title,
                "precondition": str(precondition),
                "steps": str(steps),
                "expected": str(expected),
                "type": normalize_case_type(raw_type),
                "test_data": test_data,
                "post_actions": post_actions,
            }
        )
    return norm


def generate_cases_for_feature(
    feature: Dict[str, Any],
    prd_text: str,
    guidelines: str,
    api_key: str,
    model_id: str,

) -> List[Dict[str, Any]]:
    """
    é˜¶æ®µäºŒï¼šé’ˆå¯¹å•ä¸ªåŠŸèƒ½ç‚¹ç”Ÿæˆç”¨ä¾‹
    - å¼•å…¥ä¼ä¸šæµ‹è¯•è§„èŒƒï¼ˆguidelinesï¼‰
    - è¾“å‡º JSON: {"cases":[...]}
    - æ‰€æœ‰å­—æ®µå€¼è¦æ±‚ç”¨ç®€ä½“ä¸­æ–‡
    - test_data/post_actions å­—æ®µï¼šç”¨äºåç»­è‡ªåŠ¨åŒ–æµ‹è¯•/æ¸…ç†
    - æ ¹æ® scene_type åŒºåˆ†ç­–ç•¥ï¼š
      - æ­£å‘ï¼šä¸»æµç¨‹ + å¼‚å¸¸ + è¾¹ç•Œï¼ˆå¦‚æœæœ‰ï¼‰
      - å¼‚å¸¸/çº¦æŸ/è¾¹ç•Œï¼šèšç„¦å¼‚å¸¸å’Œè¾¹ç•Œï¼Œä¸å¼ºè¡Œé€ æ— å…³æ­£å‘
    """
    guideline_text = guidelines.strip() or "æ— "
    scene_type = feature.get("scene_type", "æ­£å‘")

    # ä¸ºäº†é™ä½ä¸Šä¸‹æ–‡é•¿åº¦ï¼Œä¼˜å…ˆä½¿ç”¨åŠŸèƒ½ç‚¹è‡ªå¸¦çš„ source_textï¼Œå¹¶åšæˆªæ–­
    raw_context = feature.get("source_text") or prd_text
    context_text = raw_context[:MAX_CONTEXT_CHARS]




    if scene_type == "å¼‚å¸¸":
        coverage_text = """
    æœ¬åŠŸèƒ½ç‚¹æœ¬èº«æ˜¯å¼‚å¸¸ç±»åŠŸèƒ½ç‚¹ï¼ˆä¾‹å¦‚â€œæœªæ³¨å†Œç”¨æˆ·ç™»å½•å¤±è´¥â€ï¼‰ã€‚
    è¯·å›´ç»•è¯¥å¼‚å¸¸åœºæ™¯è®¾è®¡åˆé€‚æ•°é‡çš„ç”¨ä¾‹ï¼š
    - å¦‚æœåœºæ™¯æ¯”è¾ƒç®€å•ï¼Œå¯ä»¥åªè®¾è®¡ 1~2 æ¡å…¸å‹ç”¨ä¾‹ï¼›
    - å¦‚æœå­˜åœ¨å¤šç§é”™è¯¯ç±»å‹ã€ä¸åŒç”¨æˆ·çŠ¶æ€æˆ–æ˜æ˜¾è¾¹ç•Œæƒ…å†µï¼Œå¯ä»¥é€‚å½“å¤šå†™å‡ æ¡ï¼ˆä¾‹å¦‚ 3~5 æ¡ï¼‰ï¼›
    - è‡³å°‘è¦ä¿è¯æœ‰ 1 æ¡èƒ½ä»£è¡¨è¯¥å¼‚å¸¸åœºæ™¯çš„ç”¨ä¾‹ã€‚
    ä¸éœ€è¦ä¸ºè¯¥åŠŸèƒ½ç‚¹é¢å¤–ç”Ÿæˆâ€œç”¨æˆ·åå¯†ç å‡æ­£ç¡®æ—¶ç™»å½•æˆåŠŸâ€ä¹‹ç±»çš„æ­£å‘ç”¨ä¾‹ã€‚
    """
    elif scene_type in ("çº¦æŸ", "è¾¹ç•Œ"):
        coverage_text = """
    æœ¬åŠŸèƒ½ç‚¹å±äºçº¦æŸ/è¾¹ç•Œç±»åŠŸèƒ½ç‚¹ï¼ˆä¾‹å¦‚â€œç”¨æˆ·åé•¿åº¦å¿…é¡»åœ¨ 1~20 ä½ä»¥å†…â€ï¼‰ã€‚
    è¯·å›´ç»•è¯¥çº¦æŸ/è¾¹ç•Œè®¾è®¡åˆé€‚æ•°é‡çš„ç”¨ä¾‹ï¼š
    - è‡³å°‘ 1 æ¡ç”¨ä¾‹ä½“ç°è¾¹ç•Œå†…åˆæ³•å€¼çš„æˆåŠŸåœºæ™¯ï¼ˆä¾‹å¦‚é•¿åº¦åˆšå¥½ç­‰äºæœ€å°/æœ€å¤§å€¼æ—¶æ“ä½œæˆåŠŸï¼‰ï¼›
    - å¯ä»¥æ ¹æ®å¤æ‚åº¦ï¼Œå¢åŠ  1~3 æ¡è¶…å‡ºè¾¹ç•Œçš„å¤±è´¥åœºæ™¯ï¼ˆä¾‹å¦‚é•¿åº¦ä¸º 0 æˆ–å¤§äºæœ€å¤§é™åˆ¶æ—¶æ“ä½œå¤±è´¥ï¼‰ï¼›
    - è‹¥æŸä¸ªåœºæ™¯åŒæ—¶æ˜¯è¾¹ç•Œåˆæ˜¯å¼‚å¸¸ï¼Œåªéœ€å†™ä¸€æ¡ç”¨ä¾‹ï¼Œå¹¶ä¼˜å…ˆå°† type æ ‡è®°ä¸ºâ€œè¾¹ç•Œâ€ï¼Œä¸è¦ä¸ºåŒä¸€åœºæ™¯é‡å¤ç”Ÿæˆä¸¤æ¡ã€‚
    """
    else:
        coverage_text = f"""
    æœ¬åŠŸèƒ½ç‚¹å±äºæ­£å¸¸ä¸šåŠ¡ä¸»æµç¨‹åŠŸèƒ½ç‚¹ï¼ˆscene_type="{scene_type}"ï¼‰ã€‚
    è¯·å›´ç»•è¯¥åŠŸèƒ½ç‚¹è®¾è®¡åˆé€‚æ•°é‡çš„ç”¨ä¾‹ï¼š
    - è‡³å°‘ 1 æ¡æ ¸å¿ƒæ­£å‘æµç¨‹ç”¨ä¾‹ï¼ˆä¾‹å¦‚ï¼šè¾“å…¥åˆæ³•å‚æ•°åæ“ä½œæˆåŠŸï¼‰ï¼›
    - å¯ä»¥æ ¹æ®åŠŸèƒ½å¤æ‚åº¦ï¼Œå¢åŠ è‹¥å¹²å…¸å‹å¼‚å¸¸åœºæ™¯ï¼ˆå¦‚å¿…å¡«é¡¹ä¸ºç©ºã€æ ¼å¼é”™è¯¯ã€æƒé™ä¸è¶³ç­‰ï¼‰ï¼›
    - å¦‚æœ‰æ˜æ˜¾è¾¹ç•Œå€¼ï¼ˆé•¿åº¦/èŒƒå›´ï¼‰ï¼Œå»ºè®®è‡³å°‘åŒ…å« 1 æ¡è¾¹ç•Œç”¨ä¾‹ï¼›
    - å¦‚è§„èŒƒä¸­æåˆ°å®‰å…¨/ç•Œé¢è¦æ±‚ï¼Œå¯å¢åŠ å¯¹åº” type ä¸ºâ€œå®‰å…¨â€æˆ–â€œç•Œé¢â€çš„ç”¨ä¾‹ã€‚
    å¦‚æœæŸä¸ªåœºæ™¯åŒæ—¶æ—¢æ˜¯å¼‚å¸¸åˆæ˜¯è¾¹ç•Œï¼ˆä¾‹å¦‚â€œé•¿åº¦è¶…è¿‡æœ€å¤§å€¼æ—¶æ ¡éªŒå¤±è´¥â€ï¼‰ï¼Œè¯·åªå†™ä¸€æ¡ç”¨ä¾‹ï¼Œå¹¶ä¼˜å…ˆå°† type æ ‡è®°ä¸ºâ€œè¾¹ç•Œâ€ï¼Œä¸è¦ä¸ºåŒä¸€åœºæ™¯é‡å¤ç”Ÿæˆä¸¤æ¡ã€‚
    """

    prompt = f"""
ä½ æ˜¯ä¸€åèµ„æ·±æµ‹è¯•å·¥ç¨‹å¸ˆï¼Œè¯·é’ˆå¯¹ä¸€ä¸ªå…·ä½“åŠŸèƒ½ç‚¹è®¾è®¡æµ‹è¯•ç”¨ä¾‹ã€‚

ã€é‡è¦è¦æ±‚ã€‘
- æ‰€æœ‰å­—æ®µå†…å®¹ï¼ˆmodule/title/precondition/steps/expected/type/test_data/post_actions ç­‰ï¼‰ä¸€å¾‹ä½¿ç”¨ç®€ä½“ä¸­æ–‡ã€‚
- type å­—æ®µçš„å–å€¼å°½é‡ä½¿ç”¨ä»¥ä¸‹æšä¸¾ä¹‹ä¸€ï¼š{ALLOWED_TYPES}ã€‚
- test_data å­—æ®µç”¨äºæè¿°æœ¬ç”¨ä¾‹æ‰€éœ€çš„æµ‹è¯•æ•°æ®ï¼Œå¯ä»¥æ˜¯ç»“æ„åŒ– JSON å­—ç¬¦ä¸²ï¼ˆä¾‹å¦‚ï¼š{{"username":"test_user","password":"123456"}}ï¼‰æˆ–è‡ªç„¶è¯­è¨€æè¿°ã€‚
- post_actions å­—æ®µç”¨äºæè¿°æµ‹è¯•ç»“æŸåçš„æ¸…ç†/å›æ»šæ“ä½œï¼Œä¾‹å¦‚â€œåˆ é™¤æµ‹è¯•è´¦å·â€â€œè¿˜åŸé…ç½®â€ã€‚
- JSON çš„ key ä½¿ç”¨è‹±æ–‡ï¼Œvalue ä½¿ç”¨ä¸­æ–‡ã€‚
- ä¸è¦è¾“å‡ºä»»ä½•è§£é‡Šæ€§æ–‡å­—ï¼Œåªèƒ½è¾“å‡º JSON å¯¹è±¡ã€‚

ã€åŠŸèƒ½ç‚¹ä¿¡æ¯ã€‘
{json.dumps(feature, ensure_ascii=False, indent=2)}

ã€ä¸æœ¬åŠŸèƒ½ç‚¹æœ€ç›¸å…³çš„ PRD åŸæ–‡ç‰‡æ®µã€‘
{context_text}

ã€ä¼ä¸šæµ‹è¯•è§„èŒƒï¼ˆå¯é€‰ï¼‰ã€‘
{guideline_text}

ã€ç”¨ä¾‹è®¾è®¡ç­–ç•¥ã€‘ï¼ˆè¯·ä¸¥æ ¼éµå®ˆï¼‰
{coverage_text}

ã€è¾“å‡ºæ ¼å¼ã€‘
åªè¾“å‡º JSON å¯¹è±¡ï¼Œæ ¼å¼å¦‚ä¸‹ï¼š
{{
  "cases": [
    {{
      "id": "TC-001",
      "module": "{feature.get('module','')}",
      "title": "ç”¨ä¾‹æ ‡é¢˜ï¼ˆä¸­æ–‡ï¼‰",
      "precondition": "å‰ç½®æ¡ä»¶ï¼ˆä¸­æ–‡ï¼Œå¯ä¸ºç©ºï¼‰",
      "steps": [
        "æ­¥éª¤1ï¼ˆä¸­æ–‡ï¼‰",
        "æ­¥éª¤2ï¼ˆä¸­æ–‡ï¼‰"
      ],
      "expected": [
        "é¢„æœŸç»“æœ1ï¼ˆä¸­æ–‡ï¼‰",
        "é¢„æœŸç»“æœ2ï¼ˆä¸­æ–‡ï¼‰"
      ],
      "type": "æ­£å‘",        // ä¾‹å¦‚ï¼šæ­£å‘ / å¼‚å¸¸ / è¾¹ç•Œ / å®‰å…¨ / æ€§èƒ½ / ç•Œé¢ / å…¶ä»–
      "test_data": "æµ‹è¯•æ•°æ®æè¿°æˆ– JSON å­—ç¬¦ä¸²",
      "post_actions": "æ¸…ç†/å›æ»šæ“ä½œæè¿°ï¼ˆå¯ä¸ºç©ºå­—ç¬¦ä¸²ï¼‰"
    }}
  ]
}}
""".strip()

    messages = [
        {"role": "system", "content": "ä½ æ˜¯ä¸€åä¸¥è°¨çš„æµ‹è¯•å·¥ç¨‹å¸ˆï¼Œè¯·ç”¨ç®€ä½“ä¸­æ–‡ç¼–å†™æµ‹è¯•ç”¨ä¾‹ã€‚"},
        {"role": "user", "content": prompt},
    ]

    raw = call_llm(
        api_key=api_key,
        model_id=model_id,
        messages=messages,
        response_format={"type": "json_object"},
        timeout=240,
    )
    obj = clean_and_parse_json(raw)
    cases = normalize_cases(obj)

    for c in cases:
        c["featureId"] = feature["id"]
    return cases


def normalize_title_for_dedup(title: str) -> str:
    """
    æ ‡é¢˜å½’ä¸€åŒ–ç”¨äºå»é‡ï¼š
    - å»æ‰â€œï¼ˆè¾¹ç•Œå€¼ï¼‰â€â€œ[è¾¹ç•Œ]â€ç­‰æ ‡è®°
    - å»æ‰ç©ºç™½
    """
    t = title or ""
    t = t.replace("ï¼ˆè¾¹ç•Œå€¼ï¼‰", "")
    t = t.replace("(è¾¹ç•Œå€¼)", "")
    t = t.replace("[è¾¹ç•Œ]", "")
    t = re.sub(r"\s+", "", t)
    return t


def semantic_dedup_cases(cases: List[Dict[str, Any]], sim_threshold: float = 0.85) -> List[Dict[str, Any]]:
    """
    è¯­ä¹‰å»é‡ï¼š
    - åŒä¸€ module å†…ï¼Œå¦‚æœä¸¤æ¡ç”¨ä¾‹çš„ (title+steps) ä½™å¼¦ç›¸ä¼¼åº¦ > sim_thresholdï¼Œåˆ™è®¤ä¸ºåœºæ™¯é‡å¤
    - ä¿ç•™æè¿°æ›´è¯¦ç»†ï¼ˆsteps+expected æ›´é•¿ï¼‰çš„é‚£ä¸€æ¡
    """
    if not HAS_EMBED or EMBED_MODEL is None:
        return cases

    texts = [
        (idx, c.get("module", ""), (c.get("title", "") or "") + " " + (c.get("steps", "") or ""))
        for idx, c in enumerate(cases)
    ]
    if not texts:
        return cases

    indices, modules, contents = zip(*texts)  # type: ignore
    try:
        emb = EMBED_MODEL.encode(list(contents), convert_to_numpy=True)
    except Exception:
        return cases

    n = len(cases)
    keep = [True] * n

    for i in range(n):
        if not keep[i]:
            continue
        for j in range(i + 1, n):
            if not keep[j]:
                continue
            if modules[i] != modules[j]:
                continue
            va = emb[i]
            vb = emb[j]
            denom = (np.linalg.norm(va) + 1e-8) * (np.linalg.norm(vb) + 1e-8)
            sim = float(np.dot(va, vb) / denom)
            if sim >= sim_threshold:
                # æ¯”è¾ƒ steps+expected é•¿åº¦ï¼Œä¿ç•™æ›´è¯¦ç»†çš„é‚£æ¡
                ci = cases[i]
                cj = cases[j]
                len_i = len(ci.get("steps", "")) + len(ci.get("expected", ""))
                len_j = len(cj.get("steps", "")) + len(cj.get("expected", ""))
                if len_i >= len_j:
                    keep[j] = False
                else:
                    keep[i] = False
                    break

    return [c for idx, c in enumerate(cases) if keep[idx]]


def generate_test_cases_pipeline(
    prd_text: str,
    guidelines: str,
    api_key: str,
    model_id: str,
    progress_callback: Optional[Callable[[int, int], None]] = None,  # æ–°å¢ï¼Œç”¨äºæ›´æ–°è¿›åº¦æ¡
    enable_semantic_dedup: bool = False,  # æ–°å¢ï¼šæ˜¯å¦å¼€å¯è¯­ä¹‰å»é‡
) -> tuple[List[Dict[str, Any]], List[Dict[str, Any]]]:

    """
    æ•´ä½“ç”Ÿæˆæµç¨‹ï¼ˆåˆ†æ²»ç‰ˆï¼‰ï¼š
    1. æŠ½å–åŠŸèƒ½ç‚¹ features
    2. é’ˆå¯¹æ¯ä¸ªåŠŸèƒ½ç‚¹ï¼Œè®©æ¨¡å‹è‡ªè¡Œåˆ¤æ–­éœ€è¦å¤šå°‘æ¡ç”¨ä¾‹ï¼ˆè‡³å°‘ 1 æ¡ï¼‰
    3. æŒ‰åŠŸèƒ½ç‚¹é€ä¸ªç”Ÿæˆç”¨ä¾‹
    4. å»é‡ï¼ˆåˆå¹¶å¼‚å¸¸ + è¾¹ç•Œé‡å¤ï¼‰
    """
    features = extract_features(prd_text, guidelines, api_key, model_id)
    if not features:
        raise RuntimeError("æœªèƒ½ä» PRD ä¸­æŠ½å–åˆ°åŠŸèƒ½ç‚¹ï¼Œæ— æ³•ç”Ÿæˆç”¨ä¾‹ã€‚")

    all_cases: List[Dict[str, Any]] = []
    total = len(features)

    # âœ… ä½¿ç”¨çº¿ç¨‹æ± å¹¶å‘ä¸ºæ¯ä¸ªåŠŸèƒ½ç‚¹ç”Ÿæˆç”¨ä¾‹
    # å¯æ ¹æ®è‡ªå·±æ¥å£é™æµæƒ…å†µè°ƒæ•´ max_workers
    with concurrent.futures.ThreadPoolExecutor(max_workers=min(4, total)) as executor:
        future_to_feature = {
            executor.submit(
                generate_cases_for_feature,
                f,
                prd_text,
                guidelines,
                api_key,
                model_id,

            ): f
            for f in features
        }

        done = 0
        for future in concurrent.futures.as_completed(future_to_feature):
            f = future_to_feature[future]
            try:
                cases_f = future.result()
                all_cases.extend(cases_f)
            except Exception as e:
                print(f"ä¸ºåŠŸèƒ½ç‚¹ {f['id']} ç”Ÿæˆç”¨ä¾‹å¤±è´¥ï¼š{e}")
            finally:
                done += 1
                if progress_callback is not None:
                    progress_callback(done, total)

    # ğŸ” å…ˆåšä¸€æ¬¡ç®€å•çš„â€œæ¨¡å— + å½’ä¸€åŒ–æ ‡é¢˜â€å»é‡
    seen = {}
    dedup_cases: List[Dict[str, Any]] = []

    for c in all_cases:
        raw_title = c.get("title", "")
        norm_title = normalize_title_for_dedup(raw_title)
        key = (c.get("module", ""), norm_title)

        if key in seen:
            old_idx = seen[key]
            old_type = dedup_cases[old_idx]["type"]
            new_type = c.get("type", old_type)
            # å¦‚æœæ—§çš„æ˜¯â€œå¼‚å¸¸â€ï¼Œæ–°çš„æ˜¯â€œè¾¹ç•Œâ€ï¼Œæˆ‘ä»¬ç”¨è¾¹ç•Œè¦†ç›–å¼‚å¸¸
            if old_type == "å¼‚å¸¸" and new_type == "è¾¹ç•Œ":
                dedup_cases[old_idx]["type"] = "è¾¹ç•Œ"
            continue

        seen[key] = len(dedup_cases)
        dedup_cases.append(c)

    # â­ ç¬¬äºŒå±‚ï¼šå¯é€‰çš„è¯­ä¹‰ç›¸ä¼¼å»é‡ï¼ˆEmbeddingï¼‰
    if enable_semantic_dedup and HAS_EMBED and EMBED_MODEL is not None:
        dedup_cases = semantic_dedup_cases(dedup_cases, sim_threshold=0.85)

    return features, dedup_cases




# ================== å¿«é€Ÿæ¨¡å¼ï¼šå•è½®ç”Ÿæˆ ==================


def generate_test_cases_quick(
    prd_text: str,
    guidelines: str,
    api_key: str,
    model_id: str,
    max_cases: int = 50,
) -> tuple[List[Dict[str, Any]], List[Dict[str, Any]]]:
    """
    å¿«é€Ÿæ¨¡å¼ï¼šä¸€æ¬¡æ€§è°ƒç”¨å¤§æ¨¡å‹ç”Ÿæˆæµ‹è¯•ç”¨ä¾‹ï¼Œä¸åšåŠŸèƒ½ç‚¹æ‹†è§£ã€‚
    è¿”å›å€¼å½¢å¼ä¸ pipeline ä¸€è‡´ï¼š (features, cases)
    features è¿™é‡Œå…ˆè¿”å›ç©ºåˆ—è¡¨ []ã€‚
    """
    guideline_text = guidelines.strip() or "æ— "

    prompt = f"""
ä½ æ˜¯ä¸€åèµ„æ·±æµ‹è¯•å·¥ç¨‹å¸ˆï¼Œè¯·æ ¹æ®ä¸‹é¢çš„ PRD å†…å®¹ç›´æ¥ç”Ÿæˆæµ‹è¯•ç”¨ä¾‹ã€‚

ã€é‡è¦è¦æ±‚ã€‘
- æ‰€æœ‰å­—æ®µå†…å®¹ï¼ˆmodule/title/precondition/steps/expected/type/test_data/post_actions ç­‰ï¼‰ä¸€å¾‹ä½¿ç”¨ç®€ä½“ä¸­æ–‡ã€‚
- type å­—æ®µçš„å–å€¼å°½é‡ä½¿ç”¨ä»¥ä¸‹æšä¸¾ä¹‹ä¸€ï¼š{ALLOWED_TYPES}ã€‚
- JSON çš„ key ä½¿ç”¨è‹±æ–‡ï¼ˆå¦‚ "title"ã€"steps"ï¼‰ï¼Œvalue å¿…é¡»æ˜¯ä¸­æ–‡ã€‚
- test_data å­—æ®µç”¨äºæè¿°æœ¬ç”¨ä¾‹æ‰€éœ€çš„æµ‹è¯•æ•°æ®ï¼Œå¯ä»¥æ˜¯ç»“æ„åŒ– JSON å­—ç¬¦ä¸²ï¼ˆä¾‹å¦‚ï¼š{{"username":"test_user","password":"123456"}}ï¼‰æˆ–è‡ªç„¶è¯­è¨€æè¿°ã€‚
- post_actions å­—æ®µç”¨äºæè¿°æµ‹è¯•ç»“æŸåçš„æ¸…ç†/å›æ»šæ“ä½œï¼Œä¾‹å¦‚â€œåˆ é™¤æµ‹è¯•è´¦å·â€â€œè¿˜åŸé…ç½®â€ã€‚
- ä¸è¦è¾“å‡ºä»»ä½•è§£é‡Šæ€§æ–‡å­—ï¼Œåªèƒ½è¾“å‡º JSON å¯¹è±¡ã€‚

ã€PRD å†…å®¹ã€‘
{prd_text}

ã€ä¼ä¸šæµ‹è¯•è§„èŒƒï¼ˆå¯é€‰ï¼‰ã€‘
{guideline_text}

ã€ä»»åŠ¡è¦æ±‚ã€‘
- ç›´æ¥æ ¹æ®æ•´ä¸ª PRD è®¾è®¡æµ‹è¯•ç”¨ä¾‹ï¼Œæ•°é‡æ§åˆ¶åœ¨ä¸è¶…è¿‡ {max_cases} æ¡ã€‚
- è¦†ç›–ï¼šä¸»è¦æ­£å‘æµç¨‹ã€å…¸å‹å¼‚å¸¸åœºæ™¯ã€é‡è¦è¾¹ç•Œåœºæ™¯å’Œå…³é”®å®‰å…¨/ç•Œé¢è¦æ±‚ï¼ˆå¦‚æœè§„èŒƒä¸­æœ‰æåˆ°ï¼‰ã€‚
- æ¯æ¡ç”¨ä¾‹åªæµ‹è¯•ä¸€ä¸ªæ¸…æ™°çš„åœºæ™¯ã€‚

ã€è¾“å‡ºæ ¼å¼ã€‘
åªè¾“å‡º JSON å¯¹è±¡ï¼Œæ ¼å¼å¦‚ä¸‹ï¼š
{{
  "cases": [
    {{

      "id": "TC-001",
      "module": "æ¨¡å—åç§°ï¼ˆä¸­æ–‡ï¼‰",
      "title": "ç”¨ä¾‹æ ‡é¢˜ï¼ˆä¸­æ–‡ï¼‰",
      "precondition": "å‰ç½®æ¡ä»¶ï¼ˆä¸­æ–‡ï¼Œå¯ä¸ºç©ºå­—ç¬¦ä¸²ï¼‰",
      "steps": ["æ­¥éª¤1ï¼ˆä¸­æ–‡ï¼‰", "æ­¥éª¤2ï¼ˆä¸­æ–‡ï¼‰"],
      "expected": ["é¢„æœŸç»“æœ1ï¼ˆä¸­æ–‡ï¼‰", "é¢„æœŸç»“æœ2ï¼ˆä¸­æ–‡ï¼‰"],
      "type": "æ­£å‘",           // æ­£å‘ / å¼‚å¸¸ / è¾¹ç•Œ / å®‰å…¨ / æ€§èƒ½ / ç•Œé¢ / å…¶ä»–
      "test_data": "æµ‹è¯•æ•°æ®æè¿°æˆ– JSON å­—ç¬¦ä¸²",
      "post_actions": "æ¸…ç†/å›æ»šæ“ä½œæè¿°ï¼ˆå¯ä¸ºç©ºå­—ç¬¦ä¸²ï¼‰"
    }}
  ]
}}
""".strip()

    messages = [
        {
            "role": "system",
            "content": "ä½ æ˜¯ä¸€åèƒ½å¤Ÿå¿«é€Ÿäº§å‡ºé«˜è´¨é‡æµ‹è¯•ç”¨ä¾‹çš„èµ„æ·±æµ‹è¯•å·¥ç¨‹å¸ˆï¼Œè¯·å§‹ç»ˆä½¿ç”¨ç®€ä½“ä¸­æ–‡ç¼–å†™ç”¨ä¾‹å†…å®¹ã€‚",
        },
        {"role": "user", "content": prompt},
    ]

    raw = call_llm(
        api_key=api_key,
        model_id=model_id,
        messages=messages,
        response_format={"type": "json_object"},
        timeout=240,
    )
    obj = clean_and_parse_json(raw)
    cases = normalize_cases(obj)

    return [], cases


# ================== è¯„æµ‹ç›¸å…³å‡½æ•° ==================


def compute_basic_metrics(cases: List[Dict[str, Any]]) -> Dict[str, Any]:
    """æ ¼å¼åˆè§„ç‡ + å†—ä½™åº¦ + æ¨¡ç³Šè¯æ•°é‡"""
    total = len(cases)
    if total == 0:
        return {
            "format_rate": 0.0,
            "valid_cases": 0,
            "redundancy": 0.0,
            "unique_titles": 0,
            "vague_count": 0,
        }

    valid_count = 0
    titles = set()
    vague_words = ["ç­‰ç­‰", "å¤§æ¦‚", "å¯èƒ½", "å·¦å³", "ç›¸å…³"]
    vague_count = 0

    for c in cases:
        title = (c.get("title") or "").strip()
        steps = (c.get("steps") or "").strip()
        expected = (c.get("expected") or "").strip()

        if title and steps and expected:
            valid_count += 1

        if title:
            titles.add((c.get("module", "") + "::" + title.lower()).strip())

        content = steps + " " + expected
        for w in vague_words:
            if w in content:
                vague_count += 1

    format_rate = valid_count / total
    unique_titles = len(titles)
    redundancy = 0.0
    if total > 0:
        redundancy = max(0.0, 1.0 - unique_titles / total)

    return {
        "format_rate": format_rate,
        "valid_cases": valid_count,
        "redundancy": redundancy,
        "unique_titles": unique_titles,
        "vague_count": vague_count,
    }


def jaccard_similarity(a: str, b: str) -> float:
    """è¶…ç®€ç‰ˆ Jaccardï¼ˆå­—ç¬¦é›†åˆï¼‰ï¼Œä½œä¸ºå…œåº•"""
    set_a = set(a)
    set_b = set(b)
    if not set_a or not set_b:
        return 0.0
    inter = len(set_a & set_b)
    union = len(set_a | set_b)
    return inter / union if union else 0.0


def embedding_title_similarity(ai_cases: List[Dict[str, Any]], human_titles: List[str]) -> float:
    """
    è¯­ä¹‰ç›¸ä¼¼åº¦ï¼ˆå¯é€‰ï¼‰ï¼šåŸºäº SentenceTransformer çš„æ ‡é¢˜å‘é‡ç›¸ä¼¼åº¦
    - å¯¹æ¯æ¡äººå·¥æ ‡é¢˜ï¼Œåœ¨ AI æ ‡é¢˜ä¸­æ‰¾åˆ°æœ€é«˜ cosineï¼Œç›¸åŠ å–å¹³å‡
    """
    if not HAS_EMBED or EMBED_MODEL is None:
        raise RuntimeError("å½“å‰ç¯å¢ƒæœªå®‰è£… sentence-transformers æˆ–æ¨¡å‹åŠ è½½å¤±è´¥ã€‚")

    ai_titles = [c.get("title", "") for c in ai_cases if c.get("title")]
    if not ai_titles or not human_titles:
        return 0.0

    ai_emb = EMBED_MODEL.encode(ai_titles, convert_to_numpy=True)
    human_emb = EMBED_MODEL.encode(human_titles, convert_to_numpy=True)

    sims = []
    for h in human_emb:
        denom = np.linalg.norm(ai_emb, axis=1) * (np.linalg.norm(h) + 1e-8)
        scores = np.dot(ai_emb, h) / (denom + 1e-8)
        sims.append(float(scores.max()))
    return float(np.mean(sims)) if sims else 0.0


def evaluate_against_human_csv(ai_cases: List[Dict[str, Any]], human_df: pd.DataFrame) -> Dict[str, float]:
    """
    CSV/Excel äººå·¥ç”¨ä¾‹å¯¹æ¯”ï¼š
    - å¿…é¡»æœ‰ 'title' åˆ—
    - è¿”å›ï¼š{"jaccard":..., "semantic":..., "recall":..., "precision":..., "f1":...}
    """
    if "title" not in human_df.columns:
        raise RuntimeError("äººå·¥ç”¨ä¾‹ CSV/Excel ä¸­å¿…é¡»åŒ…å«åˆ—åä¸º 'title' çš„åˆ—")

    human_titles = [str(t) for t in human_df["title"].tolist() if str(t).strip()]
    if not human_titles:
        return {"jaccard": 0.0, "semantic": 0.0, "recall": 0.0, "precision": 0.0, "f1": 0.0}

    ai_titles = [c.get("title", "") for c in ai_cases if c.get("title")]
    ai_concat = "".join(ai_titles)
    human_concat = "".join(human_titles)
    jac = jaccard_similarity(ai_concat, human_concat) * 100

    sem = 0.0
    recall = precision = f1 = 0.0
    if HAS_EMBED and EMBED_MODEL is not None and ai_titles:
        try:
            ai_emb = EMBED_MODEL.encode(ai_titles, convert_to_numpy=True)
            human_emb = EMBED_MODEL.encode(human_titles, convert_to_numpy=True)

            # å¯¹æ¯ä¸ªäººå·¥ titleï¼Œåœ¨ AI ä¸­æ‰¾æœ€é«˜ç›¸ä¼¼åº¦
            hit_h = 0
            for h_vec in human_emb:
                denom = np.linalg.norm(ai_emb, axis=1) * (np.linalg.norm(h_vec) + 1e-8)
                scores = np.dot(ai_emb, h_vec) / (denom + 1e-8)
                if scores.max() >= 0.75:
                    hit_h += 1
            recall = hit_h / len(human_titles) if human_titles else 0.0

            # å¯¹æ¯ä¸ª AI titleï¼Œåœ¨äººå·¥ä¸­æ‰¾æœ€é«˜ç›¸ä¼¼åº¦
            hit_ai = 0
            for a_vec in ai_emb:
                denom = np.linalg.norm(human_emb, axis=1) * (np.linalg.norm(a_vec) + 1e-8)
                scores = np.dot(human_emb, a_vec) / (denom + 1e-8)
                if scores.max() >= 0.75:
                    hit_ai += 1
            precision = hit_ai / len(ai_titles) if ai_titles else 0.0

            if recall + precision > 0:
                f1 = 2 * recall * precision / (recall + precision)

            # è¯­ä¹‰ç›¸ä¼¼åº¦ï¼šäººå·¥æ ‡é¢˜å¯¹ AI æ ‡é¢˜çš„å¹³å‡æœ€é«˜ç›¸ä¼¼åº¦
            sims = []
            for h_vec in human_emb:
                denom = np.linalg.norm(ai_emb, axis=1) * (np.linalg.norm(h_vec) + 1e-8)
                scores = np.dot(ai_emb, h_vec) / (denom + 1e-8)
                sims.append(float(scores.max()))
            sem = float(sum(sims) / len(sims)) if sims else 0.0
        except Exception:
            sem = 0.0

    return {
        "jaccard": jac,
        "semantic": sem * 100,
        "recall": recall * 100,
        "precision": precision * 100,
        "f1": f1 * 100,
    }


def judge_by_llm(
    api_key: str,
    model_id: str,
    prd_text: str,
    cases: List[Dict[str, Any]],
) -> Dict[str, Any]:
    """
    LLM-as-a-Judgeï¼š
    è¾“å…¥ PRD + ç”¨ä¾‹ï¼Œè¾“å‡ºï¼š
      { completeness_score, clarity_score, overall_score, comments }
    """
    if not cases:
        raise RuntimeError("æ²¡æœ‰ç”Ÿæˆçš„ç”¨ä¾‹ï¼Œæ— æ³•è¯„å®¡")

    short_cases = [
        {
            "id": c["id"],
            "module": c["module"],
            "title": c["title"],
            "type": c["type"],
            "steps": c["steps"],
            "expected": c["expected"],
        }
        for c in cases
    ]

    prompt = f"""
ä½ ç°åœ¨æ˜¯ä¸€åéå¸¸ä¸¥æ ¼çš„æµ‹è¯•ç»ç†ï¼Œéœ€è¦å¯¹ä¸€æ‰¹ç”±å¤§æ¨¡å‹ç”Ÿæˆçš„æµ‹è¯•ç”¨ä¾‹è¿›è¡Œè´¨é‡è¯„å®¡ã€‚

ã€PRD å†…å®¹ã€‘
{prd_text}

ã€æµ‹è¯•ç”¨ä¾‹åˆ—è¡¨ï¼ˆå…³é”®ä¿¡æ¯ï¼‰ã€‘
{json.dumps(short_cases, ensure_ascii=False, indent=2)}

ã€è¯·ç»™å‡ºå¦‚ä¸‹è¯„åˆ†ï¼ˆ0~10ï¼Œæ”¯æŒä¸€ä½å°æ•°ï¼‰ã€‘ï¼š
1. completeness_scoreï¼šå®Œæ•´æ€§ã€‚æ˜¯å¦è¦†ç›–äº† PRD ä¸»è¦åŠŸèƒ½ç‚¹ä»¥åŠé‡è¦çš„å¼‚å¸¸/è¾¹ç•Œåœºæ™¯ï¼Ÿï¼ˆå¯çœ‹ä½œéœ€æ±‚è¦†ç›–ç‡çš„è¿‘ä¼¼ï¼‰
2. clarity_scoreï¼šæ¸…æ™°åº¦ã€‚ç”¨ä¾‹æè¿°æ˜¯å¦æ¸…æ™°ã€å…·ä½“ã€å¯æ‰§è¡Œï¼Ÿæ˜¯å¦å­˜åœ¨å¤§é‡æ¨¡ç³Šè¡¨è¿°ï¼Ÿ
3. overall_scoreï¼šç»¼åˆè¯„åˆ†ã€‚ç»¼åˆè€ƒè™‘å®Œæ•´æ€§ã€æ¸…æ™°åº¦ã€æ•°é‡ã€å†—ä½™ç­‰åçš„æ€»ä½“è¯„ä»·ã€‚

ã€è¾“å‡ºè¦æ±‚ã€‘
åªè¾“å‡ºä¸€ä¸ª JSON å¯¹è±¡ï¼Œä¾‹å¦‚ï¼š
{{
  "completeness_score": 8.5,
  "clarity_score": 9.0,
  "overall_score": 8.8,
  "comments": "è¿™é‡Œå†™ä½ å¯¹è¿™æ‰¹ç”¨ä¾‹çš„æ€»ä½“è¯„ä»·å’Œæ”¹è¿›å»ºè®®ã€‚"
}}
""".strip()

    messages = [
        {"role": "system", "content": "ä½ æ˜¯ä¸€åä¸¥è°¨çš„æµ‹è¯•ç»ç†ï¼Œè´Ÿè´£è¯„å®¡æµ‹è¯•ç”¨ä¾‹è´¨é‡ã€‚"},
        {"role": "user", "content": prompt},
    ]

    raw = call_llm(
        api_key=api_key,
        model_id=model_id,
        messages=messages,
        response_format={"type": "json_object"},
        timeout=240,
    )
    obj = clean_and_parse_json(raw)
    return obj


def coverage_by_llm(
    api_key: str,
    model_id: str,
    prd_text: str,
    features: List[Dict[str, Any]],
    cases: List[Dict[str, Any]],
) -> Dict[str, Any]:
    """
    ä½¿ç”¨ LLM æ£€æŸ¥â€œåŠŸèƒ½ç‚¹è¦†ç›–ç‡â€ï¼š
    - è¾“å…¥ï¼šåŠŸèƒ½ç‚¹åˆ—è¡¨ + ç”¨ä¾‹åˆ—è¡¨
    - è¾“å‡ºï¼š{coverage_score, uncovered_features, analysis}
    """
    if not features:
        raise RuntimeError("å½“å‰æ²¡æœ‰åŠŸèƒ½ç‚¹åˆ—è¡¨ï¼Œæ— æ³•è¿›è¡Œè¦†ç›–ç‡åˆ†æã€‚")

    short_features = [
        {"id": f["id"], "name": f["name"], "desc": f.get("desc", "")}
        for f in features
    ]
    short_cases = [
        {"id": c["id"], "module": c["module"], "title": c["title"], "type": c["type"]}
        for c in cases
    ]

    prompt = f"""
ä½ æ˜¯ä¸€åèµ„æ·±æµ‹è¯•ç»ç†ï¼Œéœ€è¦ä»â€œåŠŸèƒ½ç‚¹è¦†ç›–â€çš„è§’åº¦æ£€æŸ¥æµ‹è¯•ç”¨ä¾‹æ˜¯å¦å®Œæ•´ã€‚

ã€PRD å†…å®¹ã€‘
{prd_text}

ã€åŠŸèƒ½ç‚¹åˆ—è¡¨ã€‘
{json.dumps(short_features, ensure_ascii=False, indent=2)}

ã€æµ‹è¯•ç”¨ä¾‹åˆ—è¡¨ï¼ˆç®€ç‰ˆï¼‰ã€‘
{json.dumps(short_cases, ensure_ascii=False, indent=2)}

ã€ä»»åŠ¡ã€‘
- è¯·ä½ é€ä¸ªæ£€æŸ¥åŠŸèƒ½ç‚¹ï¼Œåˆ¤æ–­æ˜¯å¦è‡³å°‘æœ‰ä¸€æ¡æµ‹è¯•ç”¨ä¾‹å¯ä»¥è¦†ç›–è¯¥åŠŸèƒ½ç‚¹ã€‚
- å¦‚æœæŸä¸ªåŠŸèƒ½ç‚¹å®Œå…¨æ²¡æœ‰è¢«ä»»ä½•ç”¨ä¾‹è¦†ç›–ï¼Œè¯·å°†å®ƒè®°ä¸ºâ€œæœªè¦†ç›–åŠŸèƒ½ç‚¹â€ã€‚

ã€è¾“å‡ºè¦æ±‚ã€‘
åªè¾“å‡ºä¸€ä¸ª JSON å¯¹è±¡ï¼Œå­—æ®µåŒ…æ‹¬ï¼š
{{
  "coverage_score": 0.85,            // è¦†ç›–ç‡ = 1 - æœªè¦†ç›–åŠŸèƒ½ç‚¹æ•° / åŠŸèƒ½ç‚¹æ€»æ•°
  "uncovered_features": ["F2","F5"], // æœªè¦†ç›–çš„åŠŸèƒ½ç‚¹ id åˆ—è¡¨ï¼ˆå¦‚æ²¡æœ‰åˆ™ä¸ºç©ºæ•°ç»„ï¼‰
  "analysis": "å¯¹è¦†ç›–æƒ…å†µçš„ç®€è¦åˆ†æå’Œæ”¹è¿›å»ºè®®ï¼ˆä¸­æ–‡ï¼‰"
}}
""".strip()

    messages = [
        {"role": "system", "content": "ä½ æ˜¯ä¸€åå…³æ³¨éœ€æ±‚è¦†ç›–ç‡çš„æµ‹è¯•ç»ç†ã€‚"},
        {"role": "user", "content": prompt},
    ]

    raw = call_llm(
        api_key=api_key,
        model_id=model_id,
        messages=messages,
        response_format={"type": "json_object"},
        timeout=240,
    )
    obj = clean_and_parse_json(raw)
    return obj


def hallucination_check_by_llm(
    api_key: str,
    model_id: str,
    prd_text: str,
    cases: List[Dict[str, Any]],
    max_cases_check: int = 20,
) -> Dict[str, Any]:
    """
    å¹»è§‰æ£€æµ‹ï¼š
    - æŠ½æ ·è‹¥å¹²æ¡ç”¨ä¾‹ï¼Œè®© LLM åˆ¤æ–­â€œé¢„æœŸç»“æœâ€æ˜¯å¦æœ‰ PRD ä¾æ®
    - è¿”å›ï¼š{ suspicious_cases: [...], summary: "..." }
    """
    if not cases:
        raise RuntimeError("æ²¡æœ‰ç”¨ä¾‹ï¼Œæ— æ³•è¿›è¡Œå¹»è§‰æ£€æµ‹ã€‚")

    sample_cases = cases[:max_cases_check]
    short_cases = [
        {
            "id": c["id"],
            "module": c["module"],
            "title": c["title"],
            "steps": c["steps"],
            "expected": c["expected"],
        }
        for c in sample_cases
    ]

    prompt = f"""
ä½ æ˜¯ä¸€åéå¸¸ä¸¥è°¨çš„éœ€æ±‚åˆ†æå¸ˆï¼Œéœ€è¦æ£€æŸ¥ä»¥ä¸‹ç”±å¤§æ¨¡å‹ç”Ÿæˆçš„æµ‹è¯•ç”¨ä¾‹æ˜¯å¦å­˜åœ¨â€œå¹»è§‰â€â€”â€”å³é¢„æœŸç»“æœä¸­åŒ…å«äº† PRD ä¸­å¹¶æœªæåˆ°çš„é€»è¾‘ã€‚

ã€PRD å†…å®¹ã€‘
{prd_text}

ã€å¾…æ£€æŸ¥çš„æµ‹è¯•ç”¨ä¾‹ï¼ˆæŠ½æ ·ï¼‰ã€‘
{json.dumps(short_cases, ensure_ascii=False, indent=2)}

ã€ä»»åŠ¡ã€‘
- å¯¹æ¯æ¡ç”¨ä¾‹ï¼Œæ£€æŸ¥å…¶â€œé¢„æœŸç»“æœâ€æ˜¯å¦å¯ä»¥åœ¨ PRD ä¸­æ‰¾åˆ°ä¾æ®ã€‚
- å¦‚æœé¢„æœŸç»“æœä¸ PRD æè¿°ä¸ç¬¦ï¼Œæˆ–è€… PRD æ ¹æœ¬æ²¡æœ‰æåˆ°ç›¸å…³é€»è¾‘ï¼Œåˆ™åˆ¤å®šè¯¥ç”¨ä¾‹ä¸ºâ€œç–‘ä¼¼å¹»è§‰â€ã€‚
- æ³¨æ„ï¼šç»Ÿä¸€é”™è¯¯æ–‡æ¡ˆç­‰ç»†èŠ‚å¯ä»¥é€‚å½“å®½æ¾ï¼Œä½†ä¸èƒ½å‡­ç©ºå‡ºç°æ–°çš„ä¸šåŠ¡è§„åˆ™æˆ–æµç¨‹ã€‚

ã€è¾“å‡ºè¦æ±‚ã€‘
åªè¾“å‡ºä¸€ä¸ª JSON å¯¹è±¡ï¼Œä¾‹å¦‚ï¼š
{{
  "suspicious_cases": [
    {{"id":"TC-003","reason":"é¢„æœŸæåˆ°äº†è´¦å·é”å®šè§„åˆ™ï¼Œä½† PRD ä¸­æ²¡æœ‰ç›¸å…³æè¿°"}}
  ],
  "summary": "æ•´ä½“å¹»è§‰æ¯”ä¾‹è¾ƒä½ï¼Œå¤§éƒ¨åˆ†ç”¨ä¾‹é¢„æœŸéƒ½æœ‰ PRD ä¾æ®ã€‚"
}}
""".strip()

    messages = [
        {"role": "system", "content": "ä½ æ˜¯ä¸€åè´Ÿè´£å‘ç°å¤§æ¨¡å‹å¹»è§‰é—®é¢˜çš„éœ€æ±‚åˆ†æå¸ˆã€‚"},
        {"role": "user", "content": prompt},
    ]

    raw = call_llm(
        api_key=api_key,
        model_id=model_id,
        messages=messages,
        response_format={"type": "json_object"},
        timeout=300,
    )
    obj = clean_and_parse_json(raw)
    return obj


def improve_cases_with_llm(
    api_key: str,
    model_id: str,
    prd_text: str,
    guidelines: str,
    cases: List[Dict[str, Any]],
    judge_result: Optional[Dict[str, Any]],
) -> List[Dict[str, Any]]:
    """
    Self-Correctionï¼šæ ¹æ®è¯„å®¡æ„è§è‡ªåŠ¨ä¼˜åŒ–ç”¨ä¾‹
    - è¾“å…¥ï¼šåŸå§‹ PRDã€ä¼ä¸šæµ‹è¯•è§„èŒƒã€å½“å‰ç”¨ä¾‹ã€è¯„å®¡æ„è§ï¼ˆå¯ä¸ºç©ºï¼‰
    - è¾“å‡ºï¼šæ–°çš„ {"cases":[...]}ï¼Œå­—æ®µç»“æ„ä¸åŸå§‹ç”¨ä¾‹ä¸€è‡´
    """
    guideline_text = guidelines.strip() or "æ— "
    judge_text = json.dumps(judge_result, ensure_ascii=False, indent=2) if judge_result else "æš‚æ— è¯„å®¡æ„è§"

    prompt = f"""
ä½ æ˜¯ä¸€åèµ„æ·±æµ‹è¯•ä¸“å®¶ï¼Œç°åœ¨éœ€è¦æ ¹æ®è¯„å®¡æ„è§ï¼Œå¯¹ä¸€æ‰¹è‡ªåŠ¨ç”Ÿæˆçš„æµ‹è¯•ç”¨ä¾‹è¿›è¡Œâ€œäºŒæ¬¡ä¼˜åŒ–â€ã€‚

ã€é‡è¦è¦æ±‚ã€‘
- æ‰€æœ‰å­—æ®µå†…å®¹ï¼ˆmodule/title/precondition/steps/expected/type/test_data/post_actions ç­‰ï¼‰ä¸€å¾‹ä½¿ç”¨ç®€ä½“ä¸­æ–‡ã€‚
- type å­—æ®µçš„å–å€¼å°½é‡ä½¿ç”¨ä»¥ä¸‹æšä¸¾ä¹‹ä¸€ï¼š{ALLOWED_TYPES}ã€‚
- JSON çš„ key ä½¿ç”¨è‹±æ–‡ï¼Œvalue ä½¿ç”¨ä¸­æ–‡ã€‚
- ä¸è¦è¾“å‡ºä»»ä½•è§£é‡Šæ€§æ–‡å­—ï¼Œåªèƒ½è¾“å‡º JSON å¯¹è±¡ã€‚

ã€PRD å†…å®¹ã€‘
{prd_text}

ã€ä¼ä¸šæµ‹è¯•è§„èŒƒã€‘
{guideline_text}

ã€å½“å‰æµ‹è¯•ç”¨ä¾‹åˆ—è¡¨ã€‘
{json.dumps(cases, ensure_ascii=False, indent=2)}

ã€è¯„å®¡æ„è§ï¼ˆæ¥è‡ªæµ‹è¯•ç»ç†æˆ– LLMï¼‰ã€‘
{judge_text}

ã€ä»»åŠ¡ã€‘
- åœ¨ä¿æŒå­—æ®µç»“æ„ä¸å˜çš„å‰æä¸‹ï¼ˆid/module/title/precondition/steps/expected/type/test_data/post_actionsï¼‰ï¼Œå¯¹æµ‹è¯•ç”¨ä¾‹è¿›è¡Œä¼˜åŒ–ï¼š
  - å¯ä»¥å¯¹ç”¨ä¾‹æ ‡é¢˜ã€æ­¥éª¤ã€é¢„æœŸè¿›è¡Œæ”¹å†™ï¼Œä½¿å…¶æ›´æ¸…æ™°ã€å…·ä½“ã€å¯æ‰§è¡Œï¼›
  - å¯ä»¥åˆ é™¤æ˜æ˜¾å†—ä½™çš„ç”¨ä¾‹ï¼ˆé‡å¤æµ‹è¯•åŒä¸€åœºæ™¯ä¸”æ²¡æœ‰è¾¹ç•Œå·®å¼‚ï¼‰ï¼›
  - å¯ä»¥å¢åŠ å°‘é‡å…³é”®çš„å¼‚å¸¸/è¾¹ç•Œ/å®‰å…¨/ç•Œé¢åœºæ™¯ç”¨ä¾‹ï¼›
  - å°½é‡æå‡å®Œæ•´æ€§å’Œæ¸…æ™°åº¦ï¼ŒåŒæ—¶æ§åˆ¶å†—ä½™åº¦ã€‚
- æœ€ç»ˆè¾“å‡ºä¸€æ‰¹æ–°çš„æµ‹è¯•ç”¨ä¾‹ï¼Œæ•°é‡ä¸å½“å‰ç”¨ä¾‹å¤§è‡´ç›¸å½“ï¼ˆä¸å¿…å®Œå…¨ç›¸ç­‰ï¼‰ã€‚

ã€è¾“å‡ºæ ¼å¼ã€‘
åªè¾“å‡º JSON å¯¹è±¡ï¼Œæ ¼å¼ä¸ºï¼š
{{
  "cases": [
    {{
      "id": "TC-001",
      "module": "...",
      "title": "...",
      "precondition": "...",
      "steps": ["..."],
      "expected": ["..."],
      "type": "æ­£å‘",
      "test_data": "æµ‹è¯•æ•°æ®æè¿°æˆ– JSON å­—ç¬¦ä¸²",
      "post_actions": "æ¸…ç†/å›æ»šæ“ä½œæè¿°ï¼ˆå¯ä¸ºç©ºå­—ç¬¦ä¸²ï¼‰"
    }}
  ]
}}
""".strip()

    messages = [
        {"role": "system", "content": "ä½ æ˜¯ä¸€åèƒ½å¤Ÿæ ¹æ®è¯„å®¡æ„è§è‡ªåŠ¨ä¼˜åŒ–æµ‹è¯•ç”¨ä¾‹çš„æµ‹è¯•ä¸“å®¶ã€‚"},
        {"role": "user", "content": prompt},
    ]

    raw = call_llm(
        api_key=api_key,
        model_id=model_id,
        messages=messages,
        response_format={"type": "json_object"},
        timeout=300,
    )
    obj = clean_and_parse_json(raw)
    new_cases = normalize_cases(obj)
    return new_cases


def build_markdown_cases(cases: List[Dict[str, Any]]) -> str:
    """å¯¼å‡º Markdown ç‰ˆæµ‹è¯•ç”¨ä¾‹"""
    lines: List[str] = []
    module_map: Dict[str, List[Dict[str, Any]]] = {}
    for c in cases:
        module_map.setdefault(c["module"], []).append(c)

    for module, group in module_map.items():
        lines.append(f"## æ¨¡å—ï¼š{module}")
        lines.append("")
        for c in group:
            lines.append(f"### {c['id']} - {c['title']}")
            lines.append("")
            lines.append(f"**ç±»å‹ï¼š** {c['type']}")
            lines.append("")
            if c.get("test_data"):
                lines.append("**æµ‹è¯•æ•°æ®ï¼š**")
                lines.append(c["test_data"])
                lines.append("")
            lines.append("**å‰ç½®æ¡ä»¶ï¼š**")
            lines.append(c["precondition"] or "ï¼ˆæ— ï¼‰")
            lines.append("")
            lines.append("**æ“ä½œæ­¥éª¤ï¼š**")
            for i, s in enumerate((c["steps"] or "").splitlines(), start=1):
                lines.append(f"{i}. {s}")
            lines.append("")
            lines.append("**é¢„æœŸç»“æœï¼š**")
            for i, s in enumerate((c["expected"] or "").splitlines(), start=1):
                lines.append(f"{i}. {s}")
            lines.append("")
            if c.get("post_actions"):
                lines.append("**åç½®å¤„ç† / æ¸…ç†ï¼š**")
                for i, s in enumerate((c["post_actions"] or "").splitlines(), start=1):
                    lines.append(f"{i}. {s}")
                lines.append("")
    return "\n".join(lines)


def build_markmap_md(cases: List[Dict[str, Any]]) -> str:
    """
    MarkMap æ€ç»´å¯¼å›¾ Markdownï¼š
    # æµ‹è¯•ç”¨ä¾‹ç»“æ„
    - æ¨¡å—
      - ç”¨ä¾‹ID + æ ‡é¢˜
        - æ­¥éª¤
        - é¢„æœŸ
    """
    lines: List[str] = ["# æµ‹è¯•ç”¨ä¾‹ç»“æ„"]

    module_map: Dict[str, List[Dict[str, Any]]] = {}
    for c in cases:
        module_map.setdefault(c.get("module", "æœªåˆ†æ¨¡å—"), []).append(c)

    for module, group in module_map.items():
        lines.append(f"- {module}")
        for c in group:
            title = f"{c.get('id','')} {c.get('title','')}".strip()
            lines.append(f"  - {title}")
            for s in (c.get("steps", "") or "").splitlines()[:3]:
                s = s.strip()
                if s:
                    lines.append(f"    - æ­¥éª¤ï¼š{s}")
            for e in (c.get("expected", "") or "").splitlines()[:1]:
                e = e.strip()
                if e:
                    lines.append(f"    - é¢„æœŸï¼š{e}")

    return "\n".join(lines)


# ================== ä¾§è¾¹æ é…ç½® ==================

with st.sidebar:
    st.header("âš™ï¸ å¤§æ¨¡å‹å‚æ•°")
    ark_api_key = st.text_input(
        "ç«å±±å¼•æ“ API Key",
        type="password",
        key="ark_api_key",
        value=st.session_state.get("ark_api_key", ""),
    )
    model_id = st.text_input(
        "ç”Ÿæˆæ¨¡å‹ ID",
        value=st.session_state.get("model_id", "doubao-seed-1-6-251015"),
        key="model_id",
    )
    judge_model_id = st.text_input(
        "è¯„å®¡ / è‡ªæˆ‘ä¿®æ­£æ¨¡å‹ IDï¼ˆå¯é€‰ï¼‰",
        value=st.session_state.get("judge_model_id", "deepseek-r1-250528"),
        key="judge_model_id",
    )


    st.divider()
    st.header("ğŸ¢ ä¼ä¸šæµ‹è¯•è§„èŒƒï¼ˆRAG æ€æƒ³ï¼‰")
    test_guidelines = st.text_area(
        "æµ‹è¯•è§„èŒƒ / å†…éƒ¨è´¨é‡æ ‡å‡†ï¼ˆå¯é€‰ï¼‰",
        height=150,
        placeholder="ä¾‹å¦‚ï¼š\n- æ‰€æœ‰å¯†ç ä¼ è¾“å¿…é¡»åŠ å¯†\n- é‡‘é¢å­—æ®µä¸å¾—ä¸ºè´Ÿæ•°\n- ç®¡ç†å‘˜æ“ä½œéœ€è®°å½•å®¡è®¡æ—¥å¿—\n...",
    )

    st.divider()
    with st.expander("é£ä¹¦é…ç½® (åŠ åˆ†é¡¹)", expanded=False):
        fs_app_id = st.text_input("Feishu App ID (å¯é€‰)")
        fs_secret = st.text_input("Feishu App Secret (å¯é€‰)", type="password")
        st.caption("ä¸å¡«åˆ™ä½¿ç”¨ Mock PRD å†…å®¹ç”¨äºæ¼”ç¤ºã€‚")

# ================== å…¨å±€çŠ¶æ€ ==================

if "prd_text" not in st.session_state:
    st.session_state["prd_text"] = ""
if "features" not in st.session_state:
    st.session_state["features"] = []
if "cases" not in st.session_state:
    st.session_state["cases"] = []
if "ui_image_b64" not in st.session_state:
    st.session_state["ui_image_b64"] = None
if "judge_result" not in st.session_state:
    st.session_state["judge_result"] = None
if "coverage_result" not in st.session_state:
    st.session_state["coverage_result"] = None
if "hallucination_result" not in st.session_state:
    st.session_state["hallucination_result"] = None

# ================== é¡µé¢å¸ƒå±€ ==================

st.title("ğŸ§¬ æ™ºæµ‹ AI Pro - éœ€æ±‚é©±åŠ¨æµ‹è¯•ç”¨ä¾‹ç”Ÿæˆ & è‡ªæˆ‘ä¼˜åŒ–å¹³å°")

tab1, tab2, tab3 = st.tabs(["ğŸ“„ éœ€æ±‚è¾“å…¥", "ğŸš€ ç”¨ä¾‹ç”Ÿæˆ & å¯è§†åŒ–", "ğŸ“Š æ•ˆæœè¯„æµ‹ & è‡ªæˆ‘ä¿®æ­£"])

# -------- Tab1: éœ€æ±‚è¾“å…¥ --------
with tab1:
    col_in1, col_in2 = st.columns([2, 1])

    with col_in1:
        input_method = st.radio("é€‰æ‹©è¾“å…¥æ¥æº", ["æ–‡æœ¬ç²˜è´´", "é£ä¹¦é“¾æ¥è§£æ"], horizontal=True)
        if input_method == "æ–‡æœ¬ç²˜è´´":
            prd_text_input = st.text_area(
                "éœ€æ±‚æ–‡æ¡£å†…å®¹",
                value=st.session_state["prd_text"],
                height=320,
                placeholder="è¯·åœ¨æ­¤è¾“å…¥ PRD æ–‡æœ¬ï¼ˆæ”¯æŒ Markdownï¼‰...",
            )
            st.session_state["prd_text"] = prd_text_input
        else:
            fs_url = st.text_input("é£ä¹¦æ–‡æ¡£é“¾æ¥")
            if st.button("ğŸ” è§£æé£ä¹¦æ–‡æ¡£"):
                with st.spinner("æ­£åœ¨è°ƒç”¨é£ä¹¦ API æˆ–ä½¿ç”¨ Mock æ•°æ®..."):
                    content = get_feishu_content(fs_url, fs_app_id, fs_secret)
                    st.session_state["prd_text"] = content
                    st.success("æ–‡æ¡£è§£æå®Œæˆï¼Œå·²å†™å…¥è¾“å…¥æ¡†")
            st.text_area("è§£æåçš„ PRD å†…å®¹", st.session_state["prd_text"], height=320)

    with col_in2:
        st.markdown("#### ğŸ“¸ UI è¾…åŠ©ç”Ÿæˆï¼ˆå¤šæ¨¡æ€ï¼Œå¯é€‰ï¼‰")
        uploaded_file = st.file_uploader("ä¸Šä¼  UI è®¾è®¡å›¾ï¼ˆPNG/JPGï¼‰", type=["png", "jpg", "jpeg"])
        if uploaded_file:
            st.image(uploaded_file, caption="å·²å¯ç”¨è§†è§‰å¢å¼ºï¼ˆç›®å‰ä»…åŠ å…¥ Promptï¼‰", use_column_width=True)
            st.session_state["ui_image_b64"] = base64.b64encode(uploaded_file.getvalue()).decode()
        else:
            st.session_state["ui_image_b64"] = None

# -------- Tab2: ç”¨ä¾‹ç”Ÿæˆ & å¯è§†åŒ– --------
with tab2:
    st.subheader("2.1 æµ‹è¯•ç”¨ä¾‹ç”Ÿæˆ")

    mode = st.radio(
        "ç”Ÿæˆæ¨¡å¼",
        ["å¿«é€Ÿæ¨¡å¼ï¼ˆå•è½®ç”Ÿæˆï¼‰", "ç²¾ç»†æ¨¡å¼ï¼ˆCoT+åˆ†æ²»ï¼‰"],
        horizontal=True,
    )

    if st.button("å¼€å§‹ç”Ÿæˆæµ‹è¯•ç”¨ä¾‹", type="primary"):
        prd_text = st.session_state["prd_text"]
        if not ark_api_key:
            st.error("è¯·å…ˆåœ¨ä¾§è¾¹æ é…ç½®ç«å±±å¼•æ“ API Key")
        elif not prd_text.strip():
            st.warning("è¯·å…ˆåœ¨ Tab1 ä¸­è¾“å…¥æˆ–è§£æ PRD å†…å®¹")
        else:
            if mode.startswith("å¿«é€Ÿæ¨¡å¼"):
                with st.spinner("ğŸ¤– æ­£åœ¨å¿«é€Ÿç”Ÿæˆæµ‹è¯•ç”¨ä¾‹..."):
                    try:
                        features, cases = generate_test_cases_quick(
                            prd_text=prd_text,
                            guidelines=test_guidelines,
                            api_key=ark_api_key,
                            model_id=model_id,
                            max_cases=50,
                        )
                        st.session_state["features"] = features
                        st.session_state["cases"] = cases
                        st.success(f"âœ… [å¿«é€Ÿæ¨¡å¼] å·²ç”Ÿæˆ {len(cases)} æ¡æµ‹è¯•ç”¨ä¾‹")
                    except Exception as e:
                        st.error(f"ç”Ÿæˆè¿‡ç¨‹å‡ºé”™ï¼š{e}")
            else:
                # ç²¾ç»†æ¨¡å¼ï¼šå¢åŠ è¿›åº¦æ¡ï¼Œæ˜¾ç¤ºæ¯ä¸ªåŠŸèƒ½ç‚¹çš„å®Œæˆæƒ…å†µ
                progress_bar = st.progress(0.0)
                status_text = st.empty()

                def _progress_cb(done: int, total: int):
                    ratio = done / max(total, 1)
                    progress_bar.progress(ratio)
                    status_text.text(f"å·²å®Œæˆ {done}/{total} ä¸ªåŠŸèƒ½ç‚¹çš„ç”¨ä¾‹ç”Ÿæˆ...")

                with st.spinner("ğŸ¤– æ­£åœ¨è¿›è¡ŒåŠŸèƒ½ç‚¹æ‹†è§£å¹¶å¹¶å‘ç”Ÿæˆæµ‹è¯•ç”¨ä¾‹..."):
                    try:
                        features, cases = generate_test_cases_pipeline(
                            prd_text=prd_text,
                            guidelines=test_guidelines,
                            api_key=ark_api_key,
                            model_id=model_id,

                        )
                        st.session_state["features"] = features
                        st.session_state["cases"] = cases

                        if features:
                            avg_per_feature = len(cases) / max(len(features), 1)
                            st.success(
                                f"âœ… [ç²¾ç»†æ¨¡å¼] å·²ä¸º {len(features)} ä¸ªåŠŸèƒ½ç‚¹ç”Ÿæˆ {len(cases)} æ¡æµ‹è¯•ç”¨ä¾‹ "
                                f"(çº¦ {avg_per_feature:.1f} æ¡/åŠŸèƒ½ç‚¹)"
                            )
                        else:
                            st.success(f"âœ… [ç²¾ç»†æ¨¡å¼] å·²ç”Ÿæˆ {len(cases)} æ¡æµ‹è¯•ç”¨ä¾‹")
                    except Exception as e:
                        st.error(f"ç”Ÿæˆè¿‡ç¨‹å‡ºé”™ï¼š{e}")

    features = st.session_state["features"]
    cases = st.session_state["cases"]

    if features:
        st.markdown("### 2.2 åŠŸèƒ½ç‚¹åˆ—è¡¨ï¼ˆCoT æŠ½å–ç»“æœï¼‰")
        df_features = pd.DataFrame(features)
        st.dataframe(df_features, use_container_width=True, height=220)

    if not cases:
        st.info("å°šæœªç”Ÿæˆç”¨ä¾‹ï¼Œè¯·å…ˆç‚¹å‡»ä¸Šæ–¹æŒ‰é’®ã€‚")
    else:
        st.markdown("### 2.3 ç”¨ä¾‹è¡¨æ ¼è§†å›¾ï¼ˆæ”¯æŒç›´æ¥ç¼–è¾‘ï¼‰")

        df_cases = pd.DataFrame(cases)
        edited_df = st.data_editor(
            df_cases,
            column_config={
                "type": st.column_config.SelectboxColumn(
                    "ç±»å‹",
                    options=ALLOWED_TYPES,
                    width="small",
                ),
                "steps": st.column_config.TextColumn("æ­¥éª¤ï¼ˆå¯å¤šè¡Œï¼‰"),
                "expected": st.column_config.TextColumn("é¢„æœŸç»“æœï¼ˆå¯å¤šè¡Œï¼‰"),
                "test_data": st.column_config.TextColumn("æµ‹è¯•æ•°æ®ï¼ˆJSON æˆ–æè¿°ï¼‰"),
                "post_actions": st.column_config.TextColumn("åç½®å¤„ç† / æ¸…ç†"),
            },
            use_container_width=True,
            num_rows="dynamic",
        )
        st.session_state["cases"] = edited_df.to_dict(orient="records")

        st.markdown("#### å¯¼å‡ºç”¨ä¾‹")
        csv_bytes = edited_df.to_csv(index=False).encode("utf-8-sig")
        st.download_button(
            "ğŸ“¥ å¯¼å‡º CSV",
            data=csv_bytes,
            file_name="testcases.csv",
            mime="text/csv",
        )
        md_str = build_markdown_cases(st.session_state["cases"])
        st.download_button(
            "ğŸ“¥ å¯¼å‡º Markdownï¼ˆè¯¦ç»†ç”¨ä¾‹ï¼‰",
            data=md_str.encode("utf-8"),
            file_name="testcases.md",
            mime="text/markdown",
        )

        # Excel å¯¼å‡º
        excel_buffer = BytesIO()
        with pd.ExcelWriter(excel_buffer, engine="xlsxwriter") as writer:
            edited_df.to_excel(writer, index=False, sheet_name="TestCases")
            # å¯ä»¥åœ¨æ­¤å¤„è¿›ä¸€æ­¥è®¾ç½®å•å…ƒæ ¼æ¢è¡Œã€åˆ—å®½ç­‰
        excel_buffer.seek(0)
        st.download_button(
            "ğŸ“¥ å¯¼å‡º Excel (.xlsx)",
            data=excel_buffer,
            file_name="testcases.xlsx",
            mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
        )

        st.markdown("---")
        st.markdown("### 2.4 ç”¨ä¾‹ç»“æ„æ€ç»´å¯¼å›¾ï¼ˆMarkMap MindMapï¼‰")

        if HAS_MARKMAP:
            mm_md = build_markmap_md(st.session_state["cases"])
            markmap(mm_md, height=450)
            st.caption("æç¤ºï¼šåœ¨å›¾ä¸­å¯ä»¥ç”¨é¼ æ ‡æ‹–åŠ¨èŠ‚ç‚¹ã€æ»šè½®ç¼©æ”¾æŸ¥çœ‹æ•´ä½“ç»“æ„ã€‚")
        else:
            st.info("æœªå®‰è£… streamlit-markmapï¼Œå¦‚éœ€è„‘å›¾è¯·æ‰§è¡Œï¼š`pip install streamlit-markmap` åé‡å¯ã€‚")

# -------- Tab3: æ•ˆæœè¯„æµ‹ & è‡ªæˆ‘ä¿®æ­£ --------
with tab3:
    st.header("ğŸ“Š æ•ˆæœè¯„æµ‹ & ğŸ” è‡ªæˆ‘ä¿®æ­£ï¼ˆSelf-Correctionï¼‰")

    cases = st.session_state["cases"]
    prd_text = st.session_state["prd_text"]
    features = st.session_state["features"]

    if not cases:
        st.info("å°šæœªç”Ÿæˆç”¨ä¾‹ï¼Œè¯·åœ¨ Tab2 ä¸­å…ˆç”Ÿæˆã€‚")
    else:
        col_eval_left, col_eval_right = st.columns([1, 2])

        with col_eval_left:
            st.markdown("#### äººå·¥åŸºå‡†è¾“å…¥ï¼ˆå¯é€‰ï¼‰")
            golden_text = st.text_area(
                "äººå·¥æ ‡å‡†ç”¨ä¾‹ï¼ˆçº¯æ–‡æœ¬ï¼Œç”¨äºç²—ç•¥ç›¸ä¼¼åº¦ï¼‰",
                height=180,
                placeholder="å¯ç²˜è´´äººå·¥å†™çš„ç”¨ä¾‹æ ‡é¢˜æˆ–æ‘˜è¦ï¼Œä¸å¡«åˆ™è·³è¿‡æ­¤é¡¹ã€‚",
            )

            st.markdown("#### ä¸Šä¼ äººå·¥ç”¨ä¾‹ CSV/Excelï¼ˆå¯é€‰ï¼‰")
            st.caption("æ–‡ä»¶ä¸­éœ€è‡³å°‘åŒ…å« `title` åˆ—ã€‚")
            uploaded_gold = st.file_uploader("ä¸Šä¼ äººå·¥ç”¨ä¾‹æ–‡ä»¶", type=["csv", "xlsx", "xls"])

            run_eval = st.button("âš–ï¸ è§„åˆ™+ç»Ÿè®¡è¯„ä¼°", type="primary")
            run_judge = st.button("ğŸ§  ä½¿ç”¨ LLM è¯„å®¡")
            run_coverage = st.button("ğŸ” åŠŸèƒ½ç‚¹è¦†ç›–æ£€æŸ¥")
            run_hallu = st.button("ğŸ§¯ å¹»è§‰æ£€æŸ¥")
            run_improve = st.button("ğŸ” æ ¹æ®è¯„å®¡æ„è§è‡ªåŠ¨ä¼˜åŒ–ç”¨ä¾‹")

        with col_eval_right:
            df_cases = pd.DataFrame(cases)

            # 1. åœºæ™¯ç±»å‹åˆ†å¸ƒ
            st.subheader("1. åœºæ™¯ç±»å‹åˆ†å¸ƒ")
            if "type" in df_cases.columns:
                type_counts = df_cases["type"].value_counts().reset_index()
                type_counts.columns = ["ç±»å‹", "æ•°é‡"]
                fig_pie = px.pie(type_counts, values="æ•°é‡", names="ç±»å‹", hole=0.4)
                st.plotly_chart(fig_pie, use_container_width=True)
            else:
                st.info("ç”¨ä¾‹ä¸­æœªåŒ…å« type å­—æ®µï¼Œæ— æ³•ç»Ÿè®¡åœºæ™¯ç±»å‹åˆ†å¸ƒã€‚")

            # 2. ç»“æ„è¯„æµ‹
            metrics = compute_basic_metrics(cases)
            st.subheader("2. ç»“æ„ & æè¿°è´¨é‡ï¼ˆè§„åˆ™è¯„æµ‹ï¼‰")
            c1, c2, c3 = st.columns(3)
            c1.metric("æ ¼å¼åˆè§„ç‡", f"{metrics['format_rate'] * 100:.1f}%")
            c2.metric("å†—ä½™åº¦", f"{metrics['redundancy'] * 100:.1f}%", help="è¶Šé«˜è¯´æ˜æ ‡é¢˜é‡å¤è¶Šå¤šï¼ˆæŒ‰æ¨¡å—+æ ‡é¢˜ç»Ÿè®¡ï¼‰")
            rigor_score = max(100 - metrics["vague_count"] * 10, 0)
            c3.metric("æè¿°ä¸¥è°¨åº¦", f"{rigor_score:.1f} / 100")

            if metrics["vague_count"] > 0:
                st.warning(
                    f"æ£€æµ‹åˆ° {metrics['vague_count']} å¤„æ¨¡ç³Šè¯ï¼ˆå¦‚â€œç­‰ç­‰/å¤§æ¦‚/å¯èƒ½â€ç­‰ï¼‰ï¼Œ"
                    "å»ºè®®ä¼˜åŒ– Prompt æˆ–é€šè¿‡è‡ªæˆ‘ä¿®æ­£åŠŸèƒ½æ”¹å†™ç”¨ä¾‹ã€‚"
                )
            else:
                st.success("æœªæ£€æµ‹åˆ°æ˜æ˜¾æ¨¡ç³Šè¯ï¼Œç”¨ä¾‹æè¿°è¾ƒä¸ºä¸¥è°¨ã€‚")

            # 3. ç»Ÿè®¡+ç›¸ä¼¼åº¦é›·è¾¾å›¾
            if run_eval:
                st.markdown("---")
                st.subheader("3. ç»¼åˆé›·è¾¾å›¾è¯„ä¼°")

                # æ–‡æœ¬ Jaccardï¼ˆä½œä¸ºä¸€ä¸ªå¾ˆç²—ç³™çš„å¯¹ç…§ï¼‰
                text_sim = 0.0
                if golden_text.strip():
                    ai_concat = "".join(df_cases["title"].astype(str).tolist())
                    text_sim = jaccard_similarity(ai_concat, golden_text) * 100

                # CSV/Excel ç›¸ä¼¼åº¦ï¼ˆJaccard + è¯­ä¹‰ + F1ï¼‰
                csv_jac = csv_sem = csv_rec = csv_pre = csv_f1 = None
                if uploaded_gold is not None:
                    try:
                        if uploaded_gold.name.endswith(".csv"):
                            human_df = pd.read_csv(uploaded_gold)
                        else:
                            human_df = pd.read_excel(uploaded_gold)
                        sim_dict = evaluate_against_human_csv(cases, human_df)
                        csv_jac = sim_dict["jaccard"]
                        csv_sem = sim_dict["semantic"]
                        csv_rec = sim_dict["recall"]
                        csv_pre = sim_dict["precision"]
                        csv_f1 = sim_dict["f1"]
                        st.info(
                            f"åŸºäº CSV/Excel çš„æ ‡é¢˜ç›¸ä¼¼åº¦ï¼š"
                            f"Jaccard â‰ˆ {csv_jac:.1f}%ï¼Œ"
                            f"è¯­ä¹‰ç›¸ä¼¼åº¦ â‰ˆ {csv_sem:.1f}%ï¼Œ"
                            f"å¬å›ç‡ â‰ˆ {csv_rec:.1f}%ï¼Œ"
                            f"ç²¾ç¡®ç‡ â‰ˆ {csv_pre:.1f}%ï¼Œ"
                            f"F1 â‰ˆ {csv_f1:.1f}%ã€‚"
                        )
                    except Exception as e:
                        st.error(f"è§£æäººå·¥ç”¨ä¾‹æ–‡ä»¶å¤±è´¥ï¼š{e}")

                # ç±»å‹ä¸°å¯Œåº¦è¯„åˆ†
                if "type" in df_cases.columns:
                    types_set = set(df_cases["type"].dropna().tolist())
                else:
                    types_set = set()

                has_positive = "æ­£å‘" in types_set
                has_negative = "å¼‚å¸¸" in types_set

                if has_positive and has_negative:
                    base_score = 85.0
                elif has_positive or has_negative:
                    base_score = 70.0
                else:
                    base_score = 60.0

                extra_types = {"è¾¹ç•Œ", "å®‰å…¨", "æ€§èƒ½", "ç•Œé¢"}
                extra_count = len(types_set & extra_types)
                extra_bonus = min(15.0, extra_count * 5.0)

                balance_score = min(100.0, base_score + extra_bonus)

                format_score = metrics["format_rate"] * 100
                redundancy_score = (1 - metrics["redundancy"]) * 100
                rigor_score_final = rigor_score
                sim_score = csv_f1 if (csv_f1 is not None) else (text_sim if golden_text.strip() else 0.0)

                categories = ["åœºæ™¯ç±»å‹ä¸°å¯Œåº¦", "æ ¼å¼è§„èŒƒæ€§", "å†—ä½™æ§åˆ¶", "æè¿°ä¸¥è°¨åº¦", "å¯¹äººå·¥åŸºå‡†çš„æ¥è¿‘åº¦(F1)"]
                scores = [
                    balance_score,
                    format_score,
                    redundancy_score,
                    rigor_score_final,
                    sim_score,
                ]

                fig_radar = go.Figure()
                fig_radar.add_trace(
                    go.Scatterpolar(
                        r=scores,
                        theta=categories,
                        fill="toself",
                        name="å½“å‰ç‰ˆæœ¬",
                    )
                )
                fig_radar.update_layout(
                    polar=dict(radialaxis=dict(visible=True, range=[0, 100])),
                    showlegend=False,
                )
                st.plotly_chart(fig_radar, use_container_width=True)

            # 4. LLM è¯„å®¡
            if run_judge:
                if not ark_api_key:
                    st.error("è¯·å…ˆåœ¨ä¾§è¾¹æ é…ç½® API Key")
                elif not prd_text.strip():
                    st.error("å½“å‰ PRD ä¸ºç©ºï¼Œæ— æ³•è¿›è¡Œ LLM è¯„å®¡")
                else:
                    with st.spinner("æ­£åœ¨è°ƒç”¨è¯„å®¡æ¨¡å‹ï¼Œè¯·ç¨å€™..."):
                        try:
                            judge = judge_by_llm(ark_api_key, judge_model_id, prd_text, cases)
                            st.session_state["judge_result"] = judge
                            st.success("LLM è¯„å®¡å®Œæˆ")
                        except Exception as e:
                            st.error(f"è¯„å®¡å¤±è´¥ï¼š{e}")

            judge = st.session_state["judge_result"]
            if judge:
                st.markdown("---")
                st.subheader("4. LLM è¯„åˆ†ç»“æœï¼ˆè¿‘ä¼¼éœ€æ±‚è¦†ç›–ç‡ & æ¸…æ™°åº¦ï¼‰")
                cc1, cc2, cc3 = st.columns(3)
                cc1.metric("å®Œæ•´æ€§/éœ€æ±‚è¦†ç›–ç‡", f"{judge.get('completeness_score', 0):.1f} / 10")
                cc2.metric("æ¸…æ™°åº¦è¯„åˆ†", f"{judge.get('clarity_score', 0):.1f} / 10")
                cc3.metric("ç»¼åˆè¯„åˆ†", f"{judge.get('overall_score', 0):.1f} / 10")
                st.markdown("**LLM è¯„å®¡ç‚¹è¯„ï¼š**")
                st.write(judge.get("comments", "ï¼ˆæ¨¡å‹æœªç»™å‡ºè¯¦ç»†ç‚¹è¯„ï¼‰"))

            # 5. åŠŸèƒ½ç‚¹è¦†ç›–æ£€æŸ¥
            if run_coverage:
                if not ark_api_key:
                    st.error("è¯·å…ˆåœ¨ä¾§è¾¹æ é…ç½® API Key")
                elif not prd_text.strip():
                    st.error("å½“å‰ PRD ä¸ºç©ºï¼Œæ— æ³•è¿›è¡Œè¦†ç›–ç‡æ£€æŸ¥")
                elif not features:
                    st.error("å½“å‰æ²¡æœ‰åŠŸèƒ½ç‚¹åˆ—è¡¨ï¼Œå»ºè®®ä½¿ç”¨ç²¾ç»†æ¨¡å¼ç”Ÿæˆåå†è¿›è¡Œè¦†ç›–ç‡æ£€æŸ¥ã€‚")
                else:
                    with st.spinner("æ­£åœ¨è¿›è¡ŒåŠŸèƒ½ç‚¹è¦†ç›–ç‡åˆ†æ..."):
                        try:
                            cov = coverage_by_llm(ark_api_key, judge_model_id, prd_text, features, cases)
                            st.session_state["coverage_result"] = cov
                            st.success("è¦†ç›–ç‡åˆ†æå®Œæˆ")
                        except Exception as e:
                            st.error(f"è¦†ç›–ç‡åˆ†æå¤±è´¥ï¼š{e}")

            coverage_result = st.session_state["coverage_result"]
            if coverage_result:
                st.markdown("---")
                st.subheader("5. åŠŸèƒ½ç‚¹è¦†ç›–ç‡ï¼ˆLLM æ£€æŸ¥ç‰ˆæœ¬ï¼‰")
                cov_score = coverage_result.get("coverage_score", 0) * 100 if coverage_result.get("coverage_score", 0) <= 1.0 else coverage_result.get("coverage_score", 0)
                st.metric("åŠŸèƒ½ç‚¹è¦†ç›–ç‡", f"{cov_score:.1f}%")
                uncovered = coverage_result.get("uncovered_features", [])
                if uncovered:
                    st.warning(f"å­˜åœ¨ {len(uncovered)} ä¸ªåŠŸèƒ½ç‚¹æœªè¢«ä»»ä½•ç”¨ä¾‹è¦†ç›–ï¼š{', '.join(uncovered)}")
                st.markdown("**LLM åˆ†æè¯´æ˜ï¼š**")
                st.write(coverage_result.get("analysis", "ï¼ˆæ¨¡å‹æœªç»™å‡ºåˆ†æï¼‰"))

            # 6. å¹»è§‰æ£€æµ‹
            if run_hallu:
                if not ark_api_key:
                    st.error("è¯·å…ˆåœ¨ä¾§è¾¹æ é…ç½® API Key")
                elif not prd_text.strip():
                    st.error("å½“å‰ PRD ä¸ºç©ºï¼Œæ— æ³•è¿›è¡Œå¹»è§‰æ£€æµ‹")
                else:
                    with st.spinner("æ­£åœ¨å¯¹éƒ¨åˆ†ç”¨ä¾‹è¿›è¡Œå¹»è§‰æ£€æµ‹..."):
                        try:
                            hallu = hallucination_check_by_llm(ark_api_key, judge_model_id, prd_text, cases)
                            st.session_state["hallucination_result"] = hallu
                            st.success("å¹»è§‰æ£€æµ‹å®Œæˆ")
                        except Exception as e:
                            st.error(f"å¹»è§‰æ£€æµ‹å¤±è´¥ï¼š{e}")

            hallu = st.session_state["hallucination_result"]
            if hallu:
                st.markdown("---")
                st.subheader("6. å¹»è§‰æ£€æµ‹ç»“æœ")
                suspicious = hallu.get("suspicious_cases", [])
                if suspicious:
                    st.warning(f"æ£€æµ‹åˆ° {len(suspicious)} æ¡ç–‘ä¼¼å¹»è§‰ç”¨ä¾‹ï¼ˆé¢„æœŸç»“æœåœ¨ PRD ä¸­ç¼ºä¹ä¾æ®ï¼‰ï¼š")
                    for item in suspicious:
                        st.write(f"- {item.get('id','(æœªçŸ¥ID)')}: {item.get('reason','æœªç»™å‡ºåŸå› ')}")
                else:
                    st.success("æœªæ£€æµ‹åˆ°æ˜æ˜¾çš„å¹»è§‰ç”¨ä¾‹ã€‚")
                st.markdown("**LLM æ€»ç»“ï¼š**")
                st.write(hallu.get("summary", "ï¼ˆæ¨¡å‹æœªç»™å‡ºæ€»ç»“ï¼‰"))

            # 7. Self-Correctionï¼šæ ¹æ®è¯„å®¡æ„è§è‡ªåŠ¨ä¼˜åŒ–
            if run_improve:
                if not ark_api_key:
                    st.error("è¯·å…ˆåœ¨ä¾§è¾¹æ é…ç½® API Key")
                elif not prd_text.strip():
                    st.error("å½“å‰ PRD ä¸ºç©ºï¼Œæ— æ³•è¿›è¡Œè‡ªæˆ‘ä¿®æ­£")
                else:
                    with st.spinner("æ­£åœ¨æ ¹æ®è¯„å®¡æ„è§è‡ªåŠ¨ä¼˜åŒ–ç”¨ä¾‹..."):
                        try:
                            new_cases = improve_cases_with_llm(
                                api_key=ark_api_key,
                                model_id=judge_model_id,
                                prd_text=prd_text,
                                guidelines=test_guidelines,
                                cases=cases,
                                judge_result=st.session_state.get("judge_result"),
                            )
                            st.session_state["cases"] = new_cases
                            st.success(f"å·²ç”Ÿæˆä¼˜åŒ–åçš„ {len(new_cases)} æ¡ç”¨ä¾‹ï¼Œè¯·è¿”å› Tab2 æŸ¥çœ‹æœ€æ–°ç»“æœã€‚")
                        except Exception as e:
                            st.error(f"è‡ªæˆ‘ä¿®æ­£å¤±è´¥ï¼š{e}")
